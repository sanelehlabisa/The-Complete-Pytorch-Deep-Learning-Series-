PyTorch Deep Learning Tutorial: Implementing CNNs
Introduction to CNNs

This video covers the implementation of a basic Convolutional Neural Network (CNN) based on the LeNet-5 architecture.
The architecture includes convolutional layers, max pooling, and fully connected layers.
Data Preprocessing

MNIST images (28x28) are resized to 32x32 for the CNN.
Transforms:
Resize: Adjusts image size.
Normalize: Standardizes pixel values to have a mean of 0 and a standard deviation of 1.
Copy
from torchvision import transforms

transform = transforms.Compose([
    transforms.Resize((32, 32)),
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,))
])
Validation Set
A validation set is introduced to tune hyperparameters without using the test set.
10% of the training data is split for validation using torch.utils.data.random_split.
Copy
train_size = int(0.9 * len(train_dataset))
val_size = len(train_dataset) - train_size
train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])
CNN Architecture
The CNN consists of two convolutional layers followed by max pooling and three fully connected layers.
Each convolutional layer extracts features from the input images.

import torch.nn as nn

class LeNet5(nn.Module):
    def __init__(self):
        super(LeNet5, self).__init__()
        self.conv1 = nn.Conv2d(1, 6, kernel_size=5)  # Input: 1 channel, Output: 6 channels
        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)
        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)
        self.fc1 = nn.Linear(16 * 5 * 5, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)  # Output: 10 classes

    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))
        x = x.view(-1, 16 * 5 * 5)  # Flatten
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)  # No softmax needed here
        return x
Training Loop
The training loop is similar to previous implementations, focusing on loss calculation and optimization.
Cross-entropy loss is used for classification.

criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(model.parameters(), lr=0.01)

for epoch in range(num_epochs):
    for inputs, labels in train_loader:
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
Evaluation
After training, the model's performance is evaluated on both the validation and test sets.

# Evaluation code here
Feature Visualization
The video demonstrates how to visualize feature maps from the convolutional layers to understand what features the model is learning.

Conclusion
The CNN architecture allows for efficient feature extraction and classification of images.
The video concludes with a brief overview of the next section, which will cover more advanced architectures.