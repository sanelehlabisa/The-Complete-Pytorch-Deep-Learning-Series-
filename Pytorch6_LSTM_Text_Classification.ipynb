{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c131e741",
   "metadata": {},
   "source": [
    "# Classifying text with LSTM!\n",
    "In this Notebook we'll first introduce techniques around processing text data. The field of Natural Language Processing (NLP) has many techniques that we have not yet looked into in this series, so we'll use this notebook to introduce them so that we can look at using text data in the future! \n",
    "\n",
    "[<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/9/93/LSTM_Cell.svg/2880px-LSTM_Cell.svg.png\">](LSTM)\n",
    "<br>\n",
    "[Corresponding Tutorial Video](https://youtu.be/8K6EL7h9gYI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e84e255",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# We'll be using Pytorch's text library called torchtext! \n",
    "from torchtext.datasets import AG_NEWS\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import torchtext.transforms as T\n",
    "\n",
    "from tqdm.notebook import trange, tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d70b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameters\n",
    "learning_rate = 1e-4  # Learning rate for the optimizer\n",
    "nepochs = 20  # Number of training epochs\n",
    "batch_size = 32  # Batch size for training\n",
    "\n",
    "max_len = 128  # Maximum length of input sequences\n",
    "data_set_root = \"../../datasets\"  # Root directory of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24054589",
   "metadata": {},
   "source": [
    "## Dataset, Tokenizers and Vocab!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7364c042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the AG News Dataset\n",
    "# This dataset contains short news articles and a single label to classify the type of article\n",
    "# Note: AG_NEWS is not a PyTorch dataset class; it is a function that returns a PyTorch DataPipe\n",
    "# PyTorch DataPipes are used for data loading and processing in PyTorch\n",
    "# More information on PyTorch DataPipes: https://pytorch.org/data/main/torchdata.datapipes.iter.html\n",
    "# Difference between DataSet and DataPipe: https://medium.com/deelvin-machine-learning/comparison-of-pytorch-dataset-and-torchdata-datapipes-486e03068c58\n",
    "dataset_train = AG_NEWS(root=data_set_root, split=\"train\")  # Training dataset\n",
    "dataset_test = AG_NEWS(root=data_set_root, split=\"test\")  # Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb09877c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization is the process of splitting a text into individual words or tokens.\n",
    "# Here, we use the `get_tokenizer` function from the `torchtext.data.utils` module\n",
    "# to create a tokenizer based on the \"basic_english\" tokenization method.\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "# Define a generator function `yield_tokens` to yield tokens from the data iterator.\n",
    "# The data iterator provides pairs of (label, text) where text is the input sentence.\n",
    "def yield_tokens(data_iter):\n",
    "    for _, text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "# Build the vocabulary from the tokens yielded by the `yield_tokens` generator function.\n",
    "# We set `min_freq=2` to include a token only if it appears more than 2 times in the dataset.\n",
    "\n",
    "# We will also add \"special\" tokens that we'll use to signal something to our model\n",
    "# <pad> is a padding token that is added to the end of a sentence to ensure \n",
    "# the length of all sequences in a batch is the same\n",
    "# <sos> signals the \"Start-Of-Sentence\" aka the start of the sequence\n",
    "# <eos> signal the \"End-Of-Sentence\" aka the end of the sequence\n",
    "# <unk> \"unknown\" token is used if a token is not contained in the vocab\n",
    "vocab = build_vocab_from_iterator(\n",
    "    yield_tokens(dataset_train),  # Tokenized data iterator\n",
    "    min_freq=2,  # Minimum frequency threshold for token inclusion\n",
    "    specials=['<pad>', '<sos>', '<eos>', '<unk>'],  # Special case tokens\n",
    "    special_first=True  # Place special tokens first in the vocabulary\n",
    ")\n",
    "\n",
    "# Set the default index of the vocabulary to the index of the '<unk>' token.\n",
    "# If a token is not found in the vocabulary, it will be replaced with the '<unk>' token.\n",
    "vocab.set_default_index(vocab['<unk>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c730eb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets have a look at the vocab!\n",
    "vocab.get_itos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7a6cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example demonstrating tokenization using the tokenizer created earlier.\n",
    "# We extract the label and text of the first item in the training dataset.\n",
    "label, text = next(iter(dataset_train))\n",
    "\n",
    "# Print the original text.\n",
    "print(\"Original Text:\")\n",
    "print(text)\n",
    "\n",
    "# Tokenize the text using the tokenizer.\n",
    "print(\"\\nTokenized Text:\")\n",
    "print(tokenizer(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce12eb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a text transformation pipeline using TorchText Sequential Transform\n",
    "text_tranform = T.Sequential(\n",
    "    # Convert the sentences to indices based on the given vocabulary\n",
    "    T.VocabTransform(vocab=vocab),\n",
    "    # Add <sos> at the beginning of each sentence. 1 is used because the index for <sos> in the vocabulary is 1.\n",
    "    T.AddToken(1, begin=True),\n",
    "    # Crop the sentence if it is longer than the max length\n",
    "    T.Truncate(max_seq_len=max_len),\n",
    "    # Add <eos> at the end of each sentence. 2 is used because the index for <eos> in the vocabulary is 2.\n",
    "    T.AddToken(2, begin=False),\n",
    "    # Convert the list of lists to a tensor. This also pads a sentence with the <pad> token if it is shorter than the max length,\n",
    "    # ensuring that all sentences are the same length.\n",
    "    T.ToTensor(padding_value=0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfaf3abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a lambda function for tokenizing each batch of text data\n",
    "text_tokenizer = lambda batch: [tokenizer(x) for x in batch]\n",
    "\n",
    "# Create data loaders for training and testing\n",
    "data_loader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, num_workers=4, drop_last=True)\n",
    "data_loader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d438f735",
   "metadata": {},
   "source": [
    "## Create LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350ac2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, num_emb, output_size, num_layers=1, hidden_size=128):\n",
    "        super(LSTM, self).__init__()\n",
    "        \n",
    "        # Create an embedding layer to convert token indices to dense vectors\n",
    "        self.embedding = nn.Embedding(num_emb, hidden_size)\n",
    "        \n",
    "        # Define the LSTM layer\n",
    "        self.lstm = nn.LSTM(input_size=hidden_size, hidden_size=hidden_size, \n",
    "                            num_layers=num_layers, batch_first=True, dropout=0.5)\n",
    "        \n",
    "        # Define the output fully connected layer\n",
    "        self.fc_out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input_seq, hidden_in, mem_in):\n",
    "        # Convert token indices to dense vectors\n",
    "        input_embs = self.embedding(input_seq)\n",
    "\n",
    "        # Pass the embeddings through the LSTM layer\n",
    "        output, (hidden_out, mem_out) = self.lstm(input_embs, (hidden_in, mem_in))\n",
    "                \n",
    "        # Pass the LSTM output through the fully connected layer to get the final output\n",
    "        return self.fc_out(output), hidden_out, mem_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a482f1d7",
   "metadata": {},
   "source": [
    "## Initialise Model and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6341169c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the device to GPU if available, otherwise fallback to CPU\n",
    "device = torch.device(0 if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Define the size of the hidden layer and number of LSTM layers\n",
    "hidden_size = 64\n",
    "num_layers = 3\n",
    "\n",
    "# Create the LSTM classifier model\n",
    "lstm_classifier = LSTM(num_emb=len(vocab), output_size=4, \n",
    "                       num_layers=num_layers, hidden_size=hidden_size).to(device)\n",
    "\n",
    "# Initialize the optimizer with Adam optimizer\n",
    "optimizer = optim.Adam(lstm_classifier.parameters(), lr=learning_rate)\n",
    "\n",
    "# Define the loss function as CrossEntropyLoss for classification\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Initialize lists to store training and test loss, as well as accuracy\n",
    "training_loss_logger = []\n",
    "test_loss_logger = []\n",
    "training_acc_logger = []\n",
    "test_acc_logger = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bdb0ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how many Parameters our Model has!\n",
    "num_model_params = 0\n",
    "for param in lstm_classifier.parameters():\n",
    "    num_model_params += param.flatten().shape[0]\n",
    "\n",
    "print(\"-This Model Has %d (Approximately %d Million) Parameters!\" % (num_model_params, num_model_params//1e6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0ba82f",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd56a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tqdm progress bar for epochs\n",
    "pbar = trange(0, nepochs, leave=False, desc=\"Epoch\")\n",
    "\n",
    "# Initialize training and test accuracy\n",
    "train_acc = 0\n",
    "test_acc = 0\n",
    "\n",
    "# Loop through each epoch\n",
    "for epoch in pbar:\n",
    "    # Update progress bar description with current accuracy\n",
    "    pbar.set_postfix_str('Accuracy: Train %.2f%%, Test %.2f%%' % (train_acc * 100, test_acc * 100))\n",
    "    \n",
    "    # Set model to training mode\n",
    "    lstm_classifier.train()\n",
    "    steps = 0\n",
    "    \n",
    "    # Iterate through training data loader\n",
    "    for label, text in tqdm(data_loader_train, desc=\"Training\", leave=False):\n",
    "        bs = label.shape[0]\n",
    "        \n",
    "        # Tokenize and transform text to tensor, move to device\n",
    "        text_tokens = text_tranform(text_tokenizer(text)).to(device)\n",
    "        label = (label - 1).to(device)\n",
    "        \n",
    "        # Initialize hidden and memory states\n",
    "        hidden = torch.zeros(num_layers, bs, hidden_size, device=device)\n",
    "        memory = torch.zeros(num_layers, bs, hidden_size, device=device)\n",
    "        \n",
    "        # Forward pass through the model\n",
    "        pred, hidden, memory = lstm_classifier(text_tokens, hidden, memory)\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = loss_fn(pred[:, -1, :], label)\n",
    "            \n",
    "        # Backpropagation and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Append training loss to logger\n",
    "        training_loss_logger.append(loss.item())\n",
    "        \n",
    "        # Calculate training accuracy\n",
    "        train_acc += (pred[:, -1, :].argmax(1) == label).sum()\n",
    "        steps += bs\n",
    "        \n",
    "    # Calculate and append training accuracy for the epoch\n",
    "    train_acc = (train_acc/steps).item()\n",
    "    training_acc_logger.append(train_acc)\n",
    "    \n",
    "    # Set model to evaluation mode\n",
    "    lstm_classifier.eval()\n",
    "    steps = 0\n",
    "    \n",
    "    # Iterate through test data loader\n",
    "    with torch.no_grad():\n",
    "        for label, text in tqdm(data_loader_test, desc=\"Testing\", leave=False):\n",
    "            bs = label.shape[0]\n",
    "            # Tokenize and transform text to tensor, move to device\n",
    "            text_tokens = text_tranform(text_tokenizer(text)).to(device)\n",
    "            label = (label - 1).to(device)\n",
    "\n",
    "            # Initialize hidden and memory states\n",
    "            hidden = torch.zeros(num_layers, bs, hidden_size, device=device)\n",
    "            memory = torch.zeros(num_layers, bs, hidden_size, device=device)\n",
    "            \n",
    "            # Forward pass through the model\n",
    "            pred, hidden, memory = lstm_classifier(text_tokens, hidden, memory)\n",
    "\n",
    "            # Calculate the loss\n",
    "            loss = loss_fn(pred[:, -1, :], label)\n",
    "            test_loss_logger.append(loss.item())\n",
    "\n",
    "            # Calculate test accuracy\n",
    "            test_acc += (pred[:, -1, :].argmax(1) == label).sum()\n",
    "            steps += bs\n",
    "\n",
    "        # Calculate and append test accuracy for the epoch\n",
    "        test_acc = (test_acc/steps).item()\n",
    "        test_acc_logger.append(test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae817a76",
   "metadata": {},
   "source": [
    "## Plot Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b79069",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.figure(figsize=(10, 5))\n",
    "_ = plt.plot(np.linspace(0, nepochs, len(training_loss_logger)), training_loss_logger)\n",
    "_ = plt.plot(np.linspace(0, nepochs, len(test_loss_logger)), test_loss_logger)\n",
    "\n",
    "_ = plt.legend([\"Train\", \"Test\"])\n",
    "_ = plt.title(\"Training Vs Test Loss\")\n",
    "_ = plt.xlabel(\"Epochs\")\n",
    "_ = plt.ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f006583c",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.figure(figsize=(10, 5))\n",
    "_ = plt.plot(np.linspace(0, nepochs, len(training_acc_logger)), training_acc_logger)\n",
    "_ = plt.plot(np.linspace(0, nepochs, len(test_acc_logger)), test_acc_logger)\n",
    "\n",
    "_ = plt.legend([\"Train\", \"Test\"])\n",
    "_ = plt.title(\"Training Vs Test Accuracy\")\n",
    "_ = plt.xlabel(\"Epochs\")\n",
    "_ = plt.ylabel(\"Accuracy\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
