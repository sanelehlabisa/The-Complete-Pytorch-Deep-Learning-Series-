{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "partially initialized module 'torchvision' has no attribute 'extension' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01moptim\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset, DataLoader\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mDatasets\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtransforms\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/2025/Projects/The-Complete-Pytorch-Deep-Learning-Series-/venv/lib/python3.10/site-packages/torchvision/__init__.py:10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Don't re-order these, we need to load the _C extension (done when importing\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# .extensions) before entering _meta_registrations.\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mextension\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _HAS_OPS  \u001b[38;5;66;03m# usort:skip\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _meta_registrations, datasets, io, models, ops, transforms, utils  \u001b[38;5;66;03m# usort:skip\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/2025/Projects/The-Complete-Pytorch-Deep-Learning-Series-/venv/lib/python3.10/site-packages/torchvision/_meta_registrations.py:26\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m fn\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m wrapper\n\u001b[1;32m     25\u001b[0m \u001b[38;5;129;43m@register_meta\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mroi_align\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43mmeta_roi_align\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrois\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspatial_scale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpooled_height\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpooled_width\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_ratio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maligned\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrois\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrois must have shape as Tensor[K, 5]\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrois\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/2025/Projects/The-Complete-Pytorch-Deep-Learning-Series-/venv/lib/python3.10/site-packages/torchvision/_meta_registrations.py:18\u001b[0m, in \u001b[0;36mregister_meta.<locals>.wrapper\u001b[0;34m(fn)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(fn):\n\u001b[0;32m---> 18\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtorchvision\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextension\u001b[49m\u001b[38;5;241m.\u001b[39m_has_ops():\n\u001b[1;32m     19\u001b[0m         get_meta_lib()\u001b[38;5;241m.\u001b[39mimpl(\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mtorchvision, op_name), overload_name), fn)\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn\n",
      "\u001b[0;31mAttributeError\u001b[0m: partially initialized module 'torchvision' has no attribute 'extension' (most likely due to a circular import)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.datasets as Datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import torchvision.utils as vutils\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import math\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm.notebook import trange, tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "lr = 1e-4\n",
    "\n",
    "root = \"../datasets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "gpu_indx  = 0\n",
    "device = torch.device(gpu_indx if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our transform\n",
    "# We'll upsample the images to 32x32 as it's easier to contruct our network\n",
    "transform = transforms.Compose([transforms.Resize(32),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize([0.5], [0.5])])\n",
    "\n",
    "test_transform = transforms.Compose([transforms.Resize(32),\n",
    "                                     transforms.ToTensor(),\n",
    "                                     transforms.Normalize([0.5], [0.5])])\n",
    "\n",
    "train_set = Datasets.MNIST(root=root, train=True, transform=transform, download=True)\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size,shuffle=True, num_workers=4)\n",
    "\n",
    "test_set = Datasets.MNIST(root=root, train=False, transform=test_transform, download=True)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorQuantizer(nn.Module):\n",
    "    def __init__(self, code_book_size, embedding_dim, commitment_cost):\n",
    "        super(VectorQuantizer, self).__init__()\n",
    "        self.code_book_size = code_book_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.commitment_cost = commitment_cost\n",
    "\n",
    "        self.embedding = nn.Embedding(code_book_size, embedding_dim)\n",
    "        self.embedding.weight.data.uniform_(-1/code_book_size, 1/code_book_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        inputs = inputs.permute(0, 2, 3, 1).contiguous()  # BSxCxHxW --> BSxHxWxC\n",
    "        input_shape = inputs.shape\n",
    "        \n",
    "        flat_input = inputs.view(-1, 1, self.embedding_dim)  # BSxHxWxC --> BS*H*Wx1xC\n",
    "        \n",
    "        # Calculate the distance between each embedding and each codebook vector\n",
    "        distances = (flat_input - self.embedding.weight.unsqueeze(0)).pow(2).mean(2)  # BS*H*WxN\n",
    "        \n",
    "        # Find the closest codebook vector\n",
    "        encoding_indices = torch.argmin(distances, dim=1).unsqueeze(1)  # BS*H*Wx1\n",
    "        \n",
    "        # Select that codebook vector\n",
    "        quantized = self.embedding(encoding_indices).view(input_shape)\n",
    "        \n",
    "        # Create loss that pulls encoder embeddings and codebook vector selected\n",
    "        e_latent_loss = F.mse_loss(quantized.detach(), inputs)\n",
    "        q_latent_loss = F.mse_loss(quantized, inputs.detach())\n",
    "        loss = q_latent_loss + self.commitment_cost * e_latent_loss\n",
    "        \n",
    "        # Reconstruct quantized representation using the encoder embeddings to allow for \n",
    "        # backpropagation of gradients into encoder\n",
    "        if self.training:\n",
    "            quantized = inputs + (quantized - inputs).detach()\n",
    "        \n",
    "        return loss, quantized.permute(0, 3, 1, 2).contiguous(), encoding_indices.reshape(input_shape[0], -1)\n",
    "\n",
    "    \n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.norm1 = nn.GroupNorm(8, channels)\n",
    "        self.conv1 = nn.Conv2d(channels, channels, 3, 1, 1)\n",
    "        self.norm2 = nn.GroupNorm(8, channels)\n",
    "        self.conv2 = nn.Conv2d(channels, channels, 3, 1, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        skip = x\n",
    "        \n",
    "        x = F.elu(self.norm1(x))\n",
    "        x = F.elu(self.norm2(self.conv1(x)))\n",
    "        x = self.conv2(x) + skip\n",
    "        return x\n",
    "\n",
    "\n",
    "# We split up our network into two parts, the Encoder and the Decoder\n",
    "class DownBlock(nn.Module):\n",
    "    def __init__(self, channels_in, channels_out):\n",
    "        super(DownBlock, self).__init__()\n",
    "        self.bn1 = nn.GroupNorm(8, channels_in)\n",
    "        self.conv1 = nn.Conv2d(channels_in, channels_out, 3, 2, 1)\n",
    "        self.bn2 = nn.GroupNorm(8, channels_out)\n",
    "        self.conv2 = nn.Conv2d(channels_out, channels_out, 3, 1, 1)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(channels_in, channels_out, 3, 2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.elu(self.bn1(x))\n",
    "                  \n",
    "        x_skip = self.conv3(x)\n",
    "        \n",
    "        x = F.elu(self.bn2(self.conv1(x)))        \n",
    "        return self.conv2(x) + x_skip\n",
    "    \n",
    "    \n",
    "# We split up our network into two parts, the Encoder and the Decoder\n",
    "class UpBlock(nn.Module):\n",
    "    def __init__(self, channels_in, channels_out):\n",
    "        super(UpBlock, self).__init__()\n",
    "        self.bn1 = nn.GroupNorm(8, channels_in)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(channels_in, channels_in, 3, 1, 1)\n",
    "        self.bn2 = nn.GroupNorm(8, channels_in)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(channels_in, channels_out, 3, 1, 1)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(channels_in, channels_out, 3, 1, 1)\n",
    "        self.up_nn = nn.Upsample(scale_factor=2, mode=\"nearest\")\n",
    "\n",
    "    def forward(self, x_in):\n",
    "        x = self.up_nn(F.elu(self.bn1(x_in)))\n",
    "        \n",
    "        x_skip = self.conv3(x)\n",
    "        \n",
    "        x = F.elu(self.bn2(self.conv1(x)))\n",
    "        return self.conv2(x) + x_skip\n",
    "\n",
    "    \n",
    "# We split up our network into two parts, the Encoder and the Decoder\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, channels, ch=32, latent_channels=32):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv_1 = nn.Conv2d(channels, ch, 3, 1, 1)\n",
    "        \n",
    "        self.conv_block1 = DownBlock(ch, ch * 2)\n",
    "        self.conv_block2 = DownBlock(ch * 2, ch * 4)\n",
    "\n",
    "        # Instead of flattening (and then having to unflatten) out our feature map and \n",
    "        # putting it through a linear layer we can just use a conv layer\n",
    "        # where the kernal is the same size as the feature map \n",
    "        # (in practice it's the same thing)\n",
    "        self.res_block_1 = ResBlock(ch * 4)\n",
    "        self.res_block_2 = ResBlock(ch * 4)\n",
    "        self.res_block_3 = ResBlock(ch * 4)\n",
    "\n",
    "        self.conv_out = nn.Conv2d(4 * ch, latent_channels, 3, 1, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv_1(x)\n",
    "        \n",
    "        x = self.conv_block1(x)\n",
    "        x = self.conv_block2(x)\n",
    "\n",
    "        x = self.res_block_1(x)\n",
    "        x = self.res_block_2(x)\n",
    "        x = F.elu(self.res_block_3(x))\n",
    "\n",
    "        return self.conv_out(x)\n",
    "    \n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, channels, ch = 32, latent_channels = 32):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(latent_channels, 4 * ch, 3, 1, 1)\n",
    "        self.res_block_1 = ResBlock(ch * 4)\n",
    "        self.res_block_2 = ResBlock(ch * 4)\n",
    "        self.res_block_2 = ResBlock(ch * 4)\n",
    "\n",
    "        self.conv_block1 = UpBlock(4 * ch, 2 * ch)\n",
    "        self.conv_block2 = UpBlock(2 * ch, ch)\n",
    "        self.conv_out = nn.Conv2d(ch, channels, 3, 1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.res_block_1(x)\n",
    "        x = self.res_block_2(x)\n",
    "        x = self.res_block_2(x)\n",
    "\n",
    "        x = self.conv_block1(x)\n",
    "        x = self.conv_block2(x)\n",
    "        \n",
    "        return torch.tanh(self.conv_out(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQVAE(nn.Module):\n",
    "    def __init__(self, channel_in, ch=16, latent_channels=32, code_book_size=64, commitment_cost=0.25):\n",
    "        super(VQVAE, self).__init__()\n",
    "        self.encoder = Encoder(channels=channel_in, ch=ch, latent_channels=latent_channels)\n",
    "        \n",
    "        self.vq = VectorQuantizer(code_book_size=code_book_size, \n",
    "                                  embedding_dim=latent_channels, \n",
    "                                  commitment_cost=commitment_cost)\n",
    "        \n",
    "        self.decoder = Decoder(channels=channel_in, ch=ch, latent_channels=latent_channels)\n",
    "\n",
    "    def encode(self, x):\n",
    "        encoding = self.encoder(x)\n",
    "        vq_loss, quantized, encoding_indices = self.vq(encoding)\n",
    "        return vq_loss, quantized, encoding_indices\n",
    "        \n",
    "    def decode(self, x):\n",
    "        return self.decoder(x)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        vq_loss, quantized, encoding_indices = self.encode(x)\n",
    "        recon = self.decode(quantized)\n",
    "        \n",
    "        return recon, vq_loss, quantized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a test image\n",
    "dataiter = iter(test_loader)\n",
    "test_images = next(dataiter)[0]\n",
    "\n",
    "# View the shape\n",
    "test_images.shape\n",
    "# Visualize the data!!!\n",
    "plt.figure(figsize = (5,5))\n",
    "out = vutils.make_grid(test_images, normalize=True)\n",
    "plt.imshow(out.numpy().transpose((1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of code book embeddings\n",
    "code_book_size = 32\n",
    "\n",
    "# The number of latent embedding channels\n",
    "latent_channels = 10\n",
    "\n",
    "# Number of Training epochs\n",
    "vq_nepoch = 50\n",
    "\n",
    "# Create our network\n",
    "vae_net = VQVAE(channel_in=test_images.shape[1], latent_channels=latent_channels, ch=16, \n",
    "                code_book_size=code_book_size, commitment_cost=0.25).to(device)\n",
    "\n",
    "# Setup optimizer\n",
    "optimizer = optim.Adam(vae_net.parameters(), lr=lr)\n",
    "scaler = torch.amp.GradScaler('cuda')\n",
    "\n",
    "lr_scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=vq_nepoch, eta_min=0)\n",
    "\n",
    "# Create loss logger\n",
    "recon_loss_log = []\n",
    "qv_loss_log = []\n",
    "test_recon_loss_log = []\n",
    "train_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how many Parameters our Model has!\n",
    "num_model_params = 0\n",
    "for param in vae_net.parameters():\n",
    "    num_model_params += param.flatten().shape[0]\n",
    "\n",
    "print(\"-The VQVAE Model Has %d (Approximately %d Million) Parameters!\" % (num_model_params, \n",
    "                                                                          num_model_params//1e6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass through a test image to make sure everything is working\n",
    "recon_data, vq_loss, quantized = vae_net(test_images.to(device))\n",
    "\n",
    "# View the Latent vector shape\n",
    "quantized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbar = trange(0, vq_nepoch, leave=False, desc=\"Epoch\")   \n",
    "for epoch in pbar:\n",
    "    pbar.set_postfix_str('Loss: %.4f' % (train_loss/len(train_loader)))\n",
    "    train_loss = 0\n",
    "    vae_net.train()\n",
    "    for i, data in enumerate(tqdm(train_loader, leave=False, desc=\"Training\")):\n",
    "\n",
    "        image = data[0].to(device)\n",
    "        with torch.amp.autocast('cuda'):\n",
    "            # Forward pass the image in the data tuple\n",
    "            recon_data, vq_loss, quantized = vae_net(image)\n",
    "\n",
    "            # Calculate the loss\n",
    "            recon_loss = (recon_data - image).pow(2).mean()\n",
    "            loss = vq_loss + recon_loss\n",
    "\n",
    "        # Take a training step\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        # Log the loss\n",
    "        recon_loss_log.append(recon_loss.item())\n",
    "        qv_loss_log.append(vq_loss.item())\n",
    "        train_loss += recon_loss.item()\n",
    "        \n",
    "    lr_scheduler.step()\n",
    "\n",
    "    vae_net.eval()\n",
    "    for i, data in enumerate(tqdm(test_loader, leave=False, desc=\"Testing\")):\n",
    "        image = data[0].to(device)\n",
    "        with torch.amp.autocast('cuda'):\n",
    "            with torch.no_grad():\n",
    "                # Forward pass the image in the data tuple\n",
    "                recon_data, vq_loss, quantized = vae_net(image)\n",
    "\n",
    "                # Calculate the loss\n",
    "                recon_loss = (recon_data - image).pow(2).mean()\n",
    "                loss = vq_loss + recon_loss\n",
    "                test_recon_loss_log.append(recon_loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.linspace(0, vq_nepoch, len(recon_loss_log[200:]))\n",
    "_ = plt.plot(x_train, recon_loss_log[200:])\n",
    "\n",
    "x_test = np.linspace(0, vq_nepoch, len(test_recon_loss_log[200:]))\n",
    "_ = plt.plot(x_test, test_recon_loss_log[200:])\n",
    "_ = plt.title(\"Reconstruction Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.plot(qv_loss_log[100:])\n",
    "_ = plt.title(\"VQ Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_net.eval()\n",
    "recon_data, vq_loss, quantized = vae_net(test_images.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vq_loss, quantized, encoding_indices = vae_net.encode(test_images.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_indices[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,10))\n",
    "out = vutils.make_grid(test_images[0:8], normalize=True)\n",
    "_ = plt.imshow(out.numpy().transpose((1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,10))\n",
    "out = vutils.make_grid(recon_data.detach().cpu()[0:8], normalize=True)\n",
    "_ = plt.imshow(out.numpy().transpose((1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusoidalPosEmb(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        device = x.device\n",
    "        half_dim = self.dim // 2\n",
    "        emb = math.log(10000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
    "        emb = x[:, None] * emb[None, :]\n",
    "        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
    "        return emb\n",
    "\n",
    "# Define an Encoder module for the Transformer architecture\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, hidden_size=128, num_layers=3, num_heads=4):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        # Create multiple transformer blocks as layers\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=hidden_size, nhead=num_heads,\n",
    "                                                   dim_feedforward=hidden_size * 4, dropout=0.0,\n",
    "                                                   batch_first=True)\n",
    "        # TransformerEncoder will clone the encoder_layer \"num_layers\" times\n",
    "        self.encoder_layers = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "\n",
    "    def forward(self, input_seq, padding_mask=None):\n",
    "        \n",
    "        bs, l, h = input_seq.shape\n",
    "        # Create the causal mask\n",
    "        causal_mask = torch.triu(torch.ones(l, l, device=input_seq.device), 1).bool()\n",
    "\n",
    "        # Pass the embeddings through each transformer block\n",
    "        output = self.encoder_layers(src=input_seq, mask=causal_mask)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, num_emb, hidden_size=128, num_layers=3, num_heads=4):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        # Create an embedding layer for tokens\n",
    "        self.embedding = nn.Embedding(num_emb, hidden_size)\n",
    "\n",
    "        # Initialize sinusoidal positional embeddings\n",
    "        self.pos_emb = SinusoidalPosEmb(hidden_size)\n",
    "\n",
    "        # Create an encoder and decoder with specified parameters\n",
    "        self.encoder = Encoder(hidden_size=hidden_size, num_layers=num_layers,\n",
    "                               num_heads=num_heads)\n",
    "\n",
    "        # Define a linear layer for output prediction\n",
    "        self.fc_out = nn.Linear(hidden_size, num_emb)\n",
    "\n",
    "    def embed(self, input_seq):\n",
    "        # Embed the input sequence\n",
    "        input_embs = self.embedding(input_seq)\n",
    "        bs, l, h = input_embs.shape\n",
    "\n",
    "        # Add positional embeddings to the input embeddings\n",
    "        seq_indx = torch.arange(l, device=input_seq.device)\n",
    "        pos_emb = self.pos_emb(seq_indx).reshape(1, l, h).expand(bs, l, h)\n",
    "        embs = input_embs + pos_emb\n",
    "        return embs\n",
    "\n",
    "    def encode(self, input_seq):\n",
    "        # Embed the input sequence\n",
    "        embs = self.embed(input_seq)\n",
    "\n",
    "        # Encode the sequence\n",
    "        embs_out = self.encoder(embs)\n",
    "        return embs_out\n",
    "\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        # Encode the input sequence\n",
    "        encoded_seq = self.encode(input_seq=input_seq)\n",
    "\n",
    "        return self.fc_out(encoded_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of transformer blocks\n",
    "num_layers = 4\n",
    "\n",
    "# MultiheadAttention Heads\n",
    "num_heads = 8\n",
    "\n",
    "hidden_size = 256\n",
    "\n",
    "# Number of Training epochs\n",
    "tf_nepoch = 100\n",
    "\n",
    "# Create model\n",
    "# We'll include a \"start-sequence\" token so there are num_embeddings + 1 embeddings\n",
    "tf_generator = Transformer(num_emb=code_book_size + 1, num_layers=num_layers, \n",
    "                           hidden_size=hidden_size, num_heads=num_heads).to(device)\n",
    "\n",
    "# Initialize the optimizer with above parameters\n",
    "tf_optimizer = optim.Adam(tf_generator.parameters(), lr=lr)\n",
    "\n",
    "tf_lr_scheduler = optim.lr_scheduler.CosineAnnealingLR(tf_optimizer, T_max=tf_nepoch, eta_min=0)\n",
    "\n",
    "# Scaler for mixed precision training\n",
    "tf_scaler = torch.amp.GradScaler('cuda')\n",
    "\n",
    "# Define the loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Initialize training loss logger\n",
    "training_loss_logger = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how many Parameters our Model has!\n",
    "num_model_params = 0\n",
    "for param in tf_generator.parameters():\n",
    "    num_model_params += param.flatten().shape[0]\n",
    "\n",
    "print(\"-The TF Model Has %d (Approximately %d Million) Parameters!\" % (num_model_params, num_model_params//1e6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbar = trange(0, tf_nepoch, leave=False, desc=\"Epoch\")   \n",
    "vae_net.eval()\n",
    "for epoch in pbar:\n",
    "    pbar.set_postfix_str('Loss: %.4f' % (train_loss/len(train_loader)))\n",
    "    train_loss = 0\n",
    "    \n",
    "    tf_generator.train()\n",
    "    for i, data in enumerate(tqdm(train_loader, leave=False, desc=\"Training\")):\n",
    "        image = data[0].to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            _, _, encoding_indices = vae_net.encode(image)\n",
    "        \n",
    "        encoding_indices = encoding_indices + 1\n",
    "        tf_inputs = torch.cat((torch.zeros_like(encoding_indices[:, 0:1]), encoding_indices[:, :-1]), 1)\n",
    "        tf_outputs = encoding_indices\n",
    "\n",
    "        # Generate predictions\n",
    "        with torch.amp.autocast('cuda'):\n",
    "            pred = tf_generator(tf_inputs)\n",
    "\n",
    "        loss = loss_fn(pred.transpose(1, 2), tf_outputs)\n",
    "        \n",
    "        # Backpropagation\n",
    "        tf_optimizer.zero_grad()\n",
    "        tf_scaler.scale(loss).backward()\n",
    "        tf_scaler.step(tf_optimizer)\n",
    "        tf_scaler.update()\n",
    "\n",
    "        # Log training loss and entropy\n",
    "        training_loss_logger.append(loss.item())\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "    tf_lr_scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.plot(training_loss_logger[200:])\n",
    "_ = plt.title(\"Loss per iteration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set temperature for sampling\n",
    "temp = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to log generated tokens\n",
    "log_tokens = [torch.zeros(64, 1).long()]\n",
    "\n",
    "# Set the generator model to evaluation mode\n",
    "tf_generator.eval()\n",
    "\n",
    "# Generate tokens\n",
    "with torch.no_grad():    \n",
    "    for i in range(64):\n",
    "        # Concatenate tokens from previous iterations\n",
    "        input_tokens = torch.cat(log_tokens, 1)\n",
    "        \n",
    "        # Get model predictions for the next token\n",
    "        data_pred = tf_generator(input_tokens.to(device))\n",
    "        \n",
    "        # Sample the next token from the distribution of probabilities\n",
    "        dist = Categorical(logits=data_pred[:, -1] / temp)\n",
    "        next_tokens = dist.sample().reshape(-1, 1)\n",
    "        \n",
    "        # Append the sampled token to the list of generated tokens\n",
    "        log_tokens.append(next_tokens.cpu())\n",
    "        # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.plot(F.softmax(data_pred[0, -1], -1).flatten().cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embs_indx = torch.cat(log_tokens, 1)[:, 1:].to(device) - 1\n",
    "embeds = vae_net.vq.embedding(embs_indx).reshape(-1, 8, 8, latent_channels).permute(0, 3, 1, 2).contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_data = vae_net.decode(embeds)\n",
    "\n",
    "plt.figure(figsize = (5,5))\n",
    "out = vutils.make_grid(recon_data.detach().cpu(), normalize=True)\n",
    "_ = plt.imshow(out.numpy().transpose((1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_sample = torch.randint(code_book_size, (64, 64), device=device)\n",
    "rand_sample_embeds = vae_net.vq.embedding(rand_sample).reshape(-1, 8, 8, latent_channels).permute(0, 3, 1, 2).contiguous()\n",
    "\n",
    "recon_data = vae_net.decode(rand_sample_embeds)\n",
    "\n",
    "plt.figure(figsize = (5,5))\n",
    "out = vutils.make_grid(recon_data.detach().cpu(), normalize=True)\n",
    "_ = plt.imshow(out.numpy().transpose((1, 2, 0)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
