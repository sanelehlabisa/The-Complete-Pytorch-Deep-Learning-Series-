{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Augmentation in Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader as dataloader\n",
    "import torchvision.models as models\n",
    "\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import trange, tqdm\n",
    "import copy\n",
    "\n",
    "from Trainer import ModelTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 64\n",
    "num_epochs = 32\n",
    "learning_rate = 1e-4\n",
    "dataset_root = \"../datasets\"\n",
    "image_size = 96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_from_checkpoint = False\n",
    "save_dir = \"../data/Models\"\n",
    "model_name = \"ResNet18_STL10\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device to GPU_indx if PU is available\n",
    "GPU_indx = 0\n",
    "device = torch.device(GPU_indx if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare a composition of transforms\n",
    "# All models from the Pytorch model Zoo where trained using images normalised with \n",
    "# the mean and std (one per channel) of the whole ImageNet Dataset\n",
    "# therefore the pretrained feature \"detectors\" of the model will expect the input to \n",
    "# be normalized in the same way \n",
    "# https://pytorch.org/docs/stable/torchvision/models.html\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize(image_size),\n",
    "    #transforms.AutoAugment(),\n",
    "    transforms.ToTensor(),\n",
    "    #transforms.RandomRotation(degrees=10),\n",
    "    #transforms.RandomHorizontalFlip(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://ai.stanford.edu/~acoates/stl10/stl10_binary.tar.gz to ../datasets\\stl10_binary.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 797M/2.64G [11:56<38:58, 788kB/s]     "
     ]
    }
   ],
   "source": [
    "# Define our STL10 Datasets\n",
    "# https://pytorch.org/docs/stable/torchvision/datasets.html#torchvision.datasets.STL10\n",
    "\n",
    "# Dataset definition is a bit differenet to MNIST and CIFAR10\n",
    "# STL10 has 3 different datasets, test, train and unlabeled\n",
    "# http://ai.stanford.edu/~acoates/stl10/\n",
    "# training set only has 5000 images and test set only 8000\n",
    "# Image size in this dataset are 96x96, larger then what we've been using\n",
    "\n",
    "train_data = datasets.STL10(dataset_root, split='train', download=True, transform=train_transform)\n",
    "test_data = datasets.STL10(dataset_root, split=\"test\", download=True)\n",
    "\n",
    "# Split trainging data into train and validation set with 90/10% traning/validation split\n",
    "validation_split = 0.9\n",
    "\n",
    "n_train_examples = int(len(train_data) * validation_split)\n",
    "n_valid_examples = len(train_data) - n_train_examples\n",
    "n_train_examples = int(len(train_data)*validation_split)\n",
    "n_valid_examples = len(train_data) - n_train_examples\n",
    "\n",
    "train_data, valid_data = torch.utils.data.random_split(train_data, [n_train_examples, n_valid_examples], generator=torch.Generator().manual_seed(42))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of a classifier model\n",
    "model = models.resnet18()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load a pre-trained model!\n",
    "# SEE EXAMPLE 2 FOR MORE INFO!\n",
    "\n",
    "# pretrained_model_name = \"ResNet18_STL10_puzzle\"\n",
    "# pretrained_model_trainer = ModelTrainer(model=model, output_size=24, device=\"cpu\", \n",
    "#                                         loss_fun=nn.CrossEntropyLoss(), \n",
    "#                                         batch_size=batch_size, learning_rate=learning_rate, \n",
    "#                                         save_dir=save_dir, model_name=pretrained_model_name, \n",
    "#                                         start_from_checkpoint=True)\n",
    "\n",
    "# model = copy.deepcopy(pretrained_model_trainer.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_trainer = ModelTrainer(model=model.to(device), output_size=10, device=device, \n",
    "                             loss_fun=nn.CrossEntropyLoss(), \n",
    "                             batch_size=batch_size, learning_rate=learning_rate, \n",
    "                             save_dir=save_dir, model_name=model_name, \n",
    "                             start_from_checkpoint=start_from_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_trainer.set_data(train_set=train_data, test_set=test_data, val_set=valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment one of these and try it out!\n",
    "\n",
    "model_trainer.set_lr_schedule(optim.lr_scheduler.StepLR(model_trainer.optimizer, \n",
    "                                                         step_size=1, \n",
    "                                                         gamma=0.95))\n",
    "\n",
    "# model_trainer.set_lr_schedule(optim.lr_scheduler.CosineAnnealingLR(model_trainer.optimizer, \n",
    "#                                                                    T_max=num_epochs, \n",
    "#                                                                    eta_min=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,10))\n",
    "images, labels = next(iter(model_trainer.train_loader))\n",
    "out = torchvision.utils.make_grid(images[0:16], normalize=True)\n",
    "_ = plt.imshow(out.numpy().transpose((1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets see how many Parameter's our Model has!\n",
    "num_params = 0\n",
    "for param in model_trainer.model.parameters():\n",
    "    num_params += param.flatten().shape[0]\n",
    "print(\"This model has %d (approximately %d Million) Parameters!\" % (num_params, num_params//1e6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_trainer.run_training(num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The highest validation accuracy was %.2f%%\" %(model_trainer.best_valid_acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "_ = plt.figure(figsize = (10,5))\n",
    "train_x = np.linspace(0, num_epochs, len(model_trainer.train_loss_logger))\n",
    "_ = plt.plot(train_x, model_trainer.train_loss_logger)\n",
    "_ = plt.title(\"Training Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "_ = plt.figure(figsize = (10,5))\n",
    "train_x = np.linspace(0, num_epochs, len(model_trainer.train_acc_logger))\n",
    "_ = plt.plot(train_x, model_trainer.train_acc_logger, c = \"y\")\n",
    "valid_x = np.linspace(0, num_epochs, len(model_trainer.val_acc_logger))\n",
    "_ = plt.plot(valid_x, model_trainer.val_acc_logger, c = \"k\")\n",
    "\n",
    "_ = plt.title(\"Accuracy\")\n",
    "_ = plt.legend([\"Training accuracy\", \"Validation accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Call the evaluate function and pass the evaluation/test dataloader etc\n",
    "test_acc = model_trainer.evaluate_model(train_test_val=\"test\")\n",
    "print(\"The Test Accuracy is: %.2f%%\" %(test_acc*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
