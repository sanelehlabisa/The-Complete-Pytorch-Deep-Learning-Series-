{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import imageio\n",
    "import copy\n",
    "import math\n",
    "from tqdm.notebook import trange, tqdm\n",
    "from PIL import Image\n",
    "\n",
    "from diffusers.models import AutoencoderKL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training parameters\n",
    "batch_size = 32\n",
    "lr = 2e-5\n",
    "\n",
    "train_epoch = 1200\n",
    "\n",
    "# data_loader\n",
    "latent_size = 32\n",
    "\n",
    "data_set_root = \"../../datasets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "gpu_indx  = 1\n",
    "device = torch.device(gpu_indx if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatentDataset(Dataset):\n",
    "    def __init__(self, latent_dir):\n",
    "        self.latent_dir = latent_dir\n",
    "        self.latent_files = sorted(os.listdir(latent_dir))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.latent_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        latent_file = self.latent_files[idx]\n",
    "        latent = np.load(os.path.join(self.latent_dir, latent_file))\n",
    "        return torch.tensor(latent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set_root = \"/media/luke/Quick_Storage/Data/CelebAHQ/image_latents\"\n",
    "trainset = LatentDataset(data_set_root)\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_patches(image_tensor, patch_size=8):\n",
    "    # Get the dimensions of the image tensor\n",
    "    bs, c, h, w = image_tensor.size()\n",
    "    \n",
    "    # Define the Unfold layer with appropriate parameters\n",
    "    unfold = torch.nn.Unfold(kernel_size=patch_size, stride=patch_size)\n",
    "    \n",
    "    # Apply Unfold to the image tensor\n",
    "    unfolded = unfold(image_tensor)\n",
    "    \n",
    "    # Reshape the unfolded tensor to match the desired output shape\n",
    "    # Output shape: BSxLxH, where L is the number of patches in each dimension\n",
    "    unfolded = unfolded.transpose(1, 2).reshape(bs, -1, c * patch_size * patch_size)\n",
    "    \n",
    "    return unfolded\n",
    "\n",
    "\n",
    "def reconstruct_image(patch_sequence, image_shape, patch_size=8):\n",
    "    \"\"\"\n",
    "    Reconstructs the original image tensor from a sequence of patches.\n",
    "\n",
    "    Args:\n",
    "        patch_sequence (torch.Tensor): Sequence of patches with shape\n",
    "        BS x L x (C x patch_size x patch_size)\n",
    "        image_shape (tuple): Shape of the original image tensor (bs, c, h, w).\n",
    "        patch_size (int): Size of the patches used in extraction.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Reconstructed image tensor.\n",
    "    \"\"\"\n",
    "    bs, c, h, w = image_shape\n",
    "    num_patches_h = h // patch_size\n",
    "    num_patches_w = w // patch_size\n",
    "    \n",
    "    # Reshape the patch sequence to match the unfolded tensor shape\n",
    "    unfolded_shape = (bs, num_patches_h, num_patches_w, patch_size, patch_size, c)\n",
    "    patch_sequence = patch_sequence.view(*unfolded_shape)\n",
    "    \n",
    "    # Transpose dimensions to match the original image tensor shape\n",
    "    patch_sequence = patch_sequence.permute(0, 5, 1, 3, 2, 4).contiguous()\n",
    "    \n",
    "    # Reshape the sequence of patches back into the original image tensor shape\n",
    "    reconstructed = patch_sequence.view(bs, c, h, w)\n",
    "    \n",
    "    return reconstructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalNorm2d(nn.Module):\n",
    "    def __init__(self, hidden_size, num_features):\n",
    "        super(ConditionalNorm2d, self).__init__()\n",
    "        self.norm = nn.LayerNorm(hidden_size, elementwise_affine=False)\n",
    "\n",
    "        self.fcw = nn.Linear(num_features, hidden_size)\n",
    "        self.fcb = nn.Linear(num_features, hidden_size)\n",
    "\n",
    "    def forward(self, x, features):\n",
    "        bs, s, l = x.shape\n",
    "        \n",
    "        out = self.norm(x)\n",
    "        w = self.fcw(features).reshape(bs, 1, -1)\n",
    "        b = self.fcb(features).reshape(bs, 1, -1)\n",
    "\n",
    "        return w * out + b\n",
    "\n",
    "    \n",
    "class SinusoidalPosEmb(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        device = x.device\n",
    "        half_dim = self.dim // 2\n",
    "        emb = math.log(10000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
    "        emb = x[:, None] * emb[None, :]\n",
    "        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
    "        return emb\n",
    "    \n",
    "    \n",
    "# Transformer block with self-attention\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, hidden_size=128, num_heads=4, num_features=128):\n",
    "        # Initialize the parent nn.Module\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        \n",
    "        # Layer normalization to normalize the input data\n",
    "        self.norm = nn.LayerNorm(hidden_size)\n",
    "        \n",
    "        # Multi-head attention mechanism\n",
    "        self.multihead_attn = nn.MultiheadAttention(hidden_size, num_heads=num_heads, \n",
    "                                                    batch_first=True, dropout=0.0)\n",
    "        \n",
    "        # Another layer normalization\n",
    "        self.con_norm = ConditionalNorm2d(hidden_size, num_features)\n",
    "        \n",
    "        # Multi-layer perceptron (MLP) with a hidden layer and activation function\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size * 4),\n",
    "            nn.LayerNorm(hidden_size * 4),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(hidden_size * 4, hidden_size)\n",
    "        )\n",
    "                \n",
    "    def forward(self, x, features):\n",
    "        # Apply the first layer normalization\n",
    "        norm_x = self.norm(x)\n",
    "        \n",
    "        # Apply multi-head attention and add the input (residual connection)\n",
    "        x = self.multihead_attn(norm_x, norm_x, norm_x)[0] + x\n",
    "        \n",
    "        # Apply the second layer normalization\n",
    "        norm_x = self.con_norm(x, features)\n",
    "        \n",
    "        # Pass through the MLP and add the input (residual connection)\n",
    "        x = self.mlp(norm_x) + x\n",
    "        \n",
    "        return x\n",
    "\n",
    "    \n",
    "# Define a Vision Encoder module for the Diffusion Transformer\n",
    "class DiT(nn.Module):\n",
    "    def __init__(self, image_size, channels_in, patch_size=16, \n",
    "                 hidden_size=128, num_features=128, \n",
    "                 num_layers=3, num_heads=4):\n",
    "        super(DiT, self).__init__()\n",
    "        \n",
    "        self.time_mlp = nn.Sequential(\n",
    "            SinusoidalPosEmb(num_features),\n",
    "            nn.Linear(num_features, 2 * num_features),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(2 * num_features, num_features),\n",
    "            nn.GELU()\n",
    "        )\n",
    "        \n",
    "        self.patch_size = patch_size\n",
    "        self.fc_in = nn.Linear(channels_in * patch_size * patch_size, hidden_size)\n",
    "        \n",
    "        seq_length = (image_size // patch_size) ** 2\n",
    "        self.pos_embedding = nn.Parameter(torch.empty(1, seq_length, hidden_size).normal_(std=0.02))\n",
    "        \n",
    "        # Create multiple transformer blocks as layers\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(hidden_size, num_heads) for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.fc_out = nn.Linear(hidden_size, channels_in * patch_size * patch_size)\n",
    "                \n",
    "    def forward(self, image_in, index):  \n",
    "        # Get timestep embedding\n",
    "        index_features = self.time_mlp(index)\n",
    "\n",
    "        # Split input into patches\n",
    "        patch_seq = extract_patches(image_in, patch_size=self.patch_size)\n",
    "        patch_emb = self.fc_in(patch_seq)\n",
    "\n",
    "        # Add a unique embedding to each token embedding\n",
    "        embs = patch_emb + self.pos_embedding\n",
    "        \n",
    "        # Pass the embeddings through each Transformer block\n",
    "        for block in self.blocks:\n",
    "            embs = block(embs, index_features)\n",
    "        \n",
    "        # Project to output\n",
    "        image_out = self.fc_out(embs)\n",
    "        \n",
    "        # Reconstruct the input from patches and return result\n",
    "        return reconstruct_image(image_out, image_in.shape, patch_size=self.patch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_alphas_bar(timesteps, s=0.008):\n",
    "    steps = timesteps + 1\n",
    "    x = torch.linspace(0, steps, steps)\n",
    "    alphas_bar = torch.cos(((x / steps) + s) / (1 + s) * torch.pi * 0.5) ** 2\n",
    "    alphas_bar = alphas_bar / alphas_bar[0]\n",
    "    return alphas_bar[:timesteps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise_from_x0(curr_img, img_pred, alpha):\n",
    "    return (curr_img - alpha.sqrt() * img_pred)/((1 - alpha).sqrt() + 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cold_diffuse(diffusion_model, sample_in, total_steps, start_step=0):\n",
    "    diffusion_model.eval()\n",
    "    bs = sample_in.shape[0]\n",
    "    alphas = torch.flip(cosine_alphas_bar(total_steps), (0,)).to(device)\n",
    "    random_sample = copy.deepcopy(sample_in)\n",
    "    with torch.no_grad():\n",
    "        for i in trange(start_step, total_steps - 1):\n",
    "            index = (i * torch.ones(bs, device=sample_in.device)).long()\n",
    "\n",
    "            img_output = diffusion_model(random_sample, index)\n",
    "\n",
    "            noise = noise_from_x0(random_sample, img_output, alphas[i])\n",
    "            x0 = img_output\n",
    "\n",
    "            rep1 = alphas[i].sqrt() * x0 + (1 - alphas[i]).sqrt() * noise\n",
    "            rep2 = alphas[i + 1].sqrt() * x0 + (1 - alphas[i + 1]).sqrt() * noise\n",
    "\n",
    "            random_sample += rep2 - rep1\n",
    "\n",
    "        index = ((total_steps - 1) * torch.ones(bs, device=sample_in.device)).long()\n",
    "        img_output = diffusion_model(random_sample, index)\n",
    "\n",
    "    return img_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataloader itterable object\n",
    "dataiter = iter(train_loader)\n",
    "# Sample from the itterable object\n",
    "latents = next(dataiter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps = 500\n",
    "patch_size = 2\n",
    "\n",
    "# network\n",
    "dit = DiT(latent_size, channels_in=latents.shape[1], patch_size=patch_size, \n",
    "            hidden_size=768, num_layers=10, num_heads=8).to(device)\n",
    "\n",
    "# Adam optimizer\n",
    "optimizer = optim.Adam(dit.parameters(), lr=lr)\n",
    "\n",
    "# Scaler for mixed precision training\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "alphas = torch.flip(cosine_alphas_bar(timesteps), (0,)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how many Parameters our Model has!\n",
    "num_model_params = 0\n",
    "for param in dit.parameters():\n",
    "    num_model_params += param.flatten().shape[0]\n",
    "\n",
    "print(\"-This Model Has %d (Approximately %d Million) Parameters!\" % (num_model_params, num_model_params//1e6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_log = []\n",
    "mean_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Checkpoint\n",
    "# cp = torch.load(\"latent_dit.pt\")\n",
    "# dit.load_state_dict(cp[\"model_state_dict\"])\n",
    "# optimizer.load_state_dict(cp[\"optimizer_state_dict\"])\n",
    "# loss_log = cp[\"train_data_logger\"]\n",
    "# start_epoch = cp[\"epoch\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbar = trange(start_epoch, train_epoch, leave=False, desc=\"Epoch\")    \n",
    "dit.train()\n",
    "for epoch in pbar:\n",
    "    pbar.set_postfix_str('Loss: %.4f' % (mean_loss/len(train_loader)))\n",
    "    mean_loss = 0\n",
    "\n",
    "    for num_iter, (latents) in enumerate(tqdm(train_loader, leave=False)):\n",
    "        latents = latents.to(device)\n",
    "        \n",
    "        #the size of the current minibatch\n",
    "        bs = latents.shape[0]\n",
    "\n",
    "        rand_index = torch.randint(timesteps, (bs, ), device=device)\n",
    "        random_sample = torch.randn_like(latents)\n",
    "        alpha_batch = alphas[rand_index].reshape(bs, 1, 1, 1)\n",
    "        \n",
    "        noise_input = alpha_batch.sqrt() * latents +\\\n",
    "        (1 - alpha_batch).sqrt() * random_sample\n",
    "        \n",
    "        with torch.cuda.amp.autocast():\n",
    "            latent_pred = dit(noise_input, rand_index)\n",
    "            loss = F.l1_loss(latent_pred, latents)\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        #log the generator training loss\n",
    "        loss_log.append(loss.item())\n",
    "        mean_loss += loss.item()\n",
    "\n",
    "    # Quick save of the model every epoch\n",
    "    torch.save({'epoch': epoch + 1,\n",
    "                'train_data_logger': loss_log,\n",
    "                'model_state_dict': dit.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                 }, \"latent_dit.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss\n",
    "plt.plot(loss_log[1000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = AutoencoderKL.from_pretrained(\"stabilityai/sd-vae-ft-ema\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_noise = 0.95 * torch.randn(8, 4, latent_size, latent_size, device=device)\n",
    "with torch.no_grad():\n",
    "    with torch.cuda.amp.autocast():\n",
    "        fake_latents = cold_diffuse(dit, latent_noise, total_steps=timesteps)\n",
    "        fake_sample = vae.decode(fake_latents / 0.18215).sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20, 10))\n",
    "out = vutils.make_grid(fake_sample.detach().float().cpu(), nrow=4, normalize=True)\n",
    "_ = plt.imshow(out.numpy().transpose((1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup data:\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(256),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5], inplace=True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a test image!\n",
    "test_img = Image.open(\"../data/obama.png\")\n",
    "test_tensor = transform(test_img).unsqueeze(0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many different images to create\n",
    "mini_batch_size = 8\n",
    "\n",
    "# Where is the forward process to start from (0-499)\n",
    "# The closer to the start (0) the more noise added to the source image!\n",
    "index = 300\n",
    "\n",
    "with torch.no_grad():\n",
    "    with torch.cuda.amp.autocast():\n",
    "        latents = vae.encode(test_tensor).latent_dist.sample().mul_(0.18215)\n",
    "        latents = latents.expand(mini_batch_size, 4, latent_size, latent_size)\n",
    "        latent_noise = 0.95 * torch.randn_like(latents)\n",
    "\n",
    "        alpha_batch = alphas[index].expand(mini_batch_size).reshape(mini_batch_size, \n",
    "                                                                    1, 1, 1)\n",
    "        noise_input = alpha_batch.sqrt() * latents +\\\n",
    "        (1 - alpha_batch).sqrt() * latent_noise\n",
    "        \n",
    "        fake_latents = cold_diffuse(dit, noise_input, \n",
    "                                    total_steps=timesteps, \n",
    "                                    start_step=index)\n",
    "        \n",
    "        fake_sample = vae.decode(fake_latents / 0.18215).sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (5, 5))\n",
    "comb_samples = torch.cat((test_tensor, fake_sample.detach()), 0)\n",
    "out = vutils.make_grid(comb_samples.detach().float().cpu(), nrow=3, normalize=True)\n",
    "_ = plt.imshow(out.numpy().transpose((1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cold_diffuse_inpaint(diffusion_model, sample_in, target, mask, \n",
    "                         total_steps, start_step=0):\n",
    "    diffusion_model.eval()\n",
    "    bs = sample_in.shape[0]\n",
    "    alphas = torch.flip(cosine_alphas_bar(total_steps), (0,)).to(device)\n",
    "    random_sample = copy.deepcopy(sample_in)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in trange(start_step, total_steps - 1):\n",
    "            index = (i * torch.ones(bs, device=sample_in.device)).long()\n",
    "\n",
    "            # noising up the target to match the same step in the process\n",
    "            noisy_target = alphas[i].sqrt() * target +\\\n",
    "            (1 - alphas[i]).sqrt() * torch.randn_like(target)\n",
    "                \n",
    "            # Use the mask to replace certain parts of the generation with the \n",
    "            # noisy target\n",
    "            random_sample = mask * random_sample + (1 - mask) * noisy_target\n",
    "            \n",
    "            img_output = diffusion_model(random_sample, index)\n",
    "\n",
    "            noise = noise_from_x0(random_sample, img_output, alphas[i])\n",
    "            x0 = img_output\n",
    "\n",
    "            rep1 = alphas[i].sqrt() * x0 + (1 - alphas[i]).sqrt() * noise\n",
    "            rep2 = alphas[i + 1].sqrt() * x0 + (1 - alphas[i + 1]).sqrt() * noise\n",
    "            \n",
    "            # The current latent in the diffusion generation process\n",
    "            random_sample += rep2 - rep1\n",
    "            \n",
    "        index = ((total_steps - 1) * torch.ones(bs, device=sample_in.device)).long()\n",
    "        img_output = diffusion_model(random_sample, index)\n",
    "\n",
    "    return img_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hard code a simple mask\n",
    "# A value of 1 means that, that part will be replaced\n",
    "# A value of 0 means that, that part will be kept\n",
    "mask = torch.zeros(1, latent_size, latent_size, device=device)\n",
    "mask[:, 8:latent_size-8, 8:latent_size-8] = torch.ones(16, 16, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will simply replace a square region in the middle of the image!\n",
    "_ = plt.imshow(mask[0].cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_batch_size = 8\n",
    "\n",
    "with torch.no_grad():\n",
    "    with torch.cuda.amp.autocast():\n",
    "        latents = vae.encode(test_tensor).latent_dist.sample().mul_(0.18215)\n",
    "        latents = latents.expand(mini_batch_size, 4, latent_size, latent_size)\n",
    "        noise_input = 0.9 * torch.randn_like(latents)\n",
    "\n",
    "        fake_latents = cold_diffuse_inpaint(dit, \n",
    "                                            noise_input, \n",
    "                                            total_steps=timesteps,\n",
    "                                            target=latents,\n",
    "                                            mask=mask)\n",
    "        \n",
    "        fake_sample = vae.decode(fake_latents / 0.18215).sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (5, 5))\n",
    "comb_samples = torch.cat((test_tensor, fake_sample.detach()), 0)\n",
    "out = vutils.make_grid(comb_samples.detach().float().cpu(), nrow=3, normalize=True)\n",
    "_ = plt.imshow(out.numpy().transpose((1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_batch_size = 8\n",
    "\n",
    "with torch.no_grad():\n",
    "    with torch.cuda.amp.autocast():\n",
    "        latents = vae.encode(test_tensor).latent_dist.sample().mul_(0.18215)\n",
    "        latents = latents.expand(mini_batch_size, 4, latent_size, latent_size)\n",
    "        noise_input = 0.9 * torch.randn_like(latents)\n",
    "\n",
    "        fake_latents = cold_diffuse_inpaint(dit, \n",
    "                                            noise_input, \n",
    "                                            total_steps=timesteps,\n",
    "                                            target=latents,\n",
    "                                            mask=1 - mask)  # Invert the mask\n",
    "        \n",
    "        fake_sample = vae.decode(fake_latents / 0.18215).sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (5, 5))\n",
    "comb_samples = torch.cat((test_tensor, fake_sample.detach()), 0)\n",
    "out = vutils.make_grid(comb_samples.detach().float().cpu(), nrow=3, normalize=True)\n",
    "_ = plt.imshow(out.numpy().transpose((1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup data:\n",
    "mask_transform = transforms.Compose([\n",
    "    transforms.Resize(32),\n",
    "    transforms.CenterCrop(32),\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_mask = Image.open(\"../data/ob_face_mask.png\")\n",
    "face_mask_tensor = mask_transform(face_mask).unsqueeze(0).to(device)\n",
    "\n",
    "bg_mask = Image.open(\"../data/ob_bg_mask.png\")\n",
    "bg_mask_tensor = mask_transform(bg_mask).unsqueeze(0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mask of just the face region!\n",
    "_ = plt.imshow(face_mask_tensor[0, 0].cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mask of the whole body\n",
    "_ = plt.imshow(bg_mask_tensor[0, 0].cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_batch_size = 8\n",
    "\n",
    "with torch.no_grad():\n",
    "    with torch.cuda.amp.autocast():\n",
    "        latents = vae.encode(test_tensor).latent_dist.sample().mul_(0.18215)\n",
    "        latents = latents.expand(mini_batch_size, 4, latent_size, latent_size)\n",
    "        noise_input = 0.9 * torch.randn_like(latents)\n",
    "\n",
    "        fake_latents = cold_diffuse_inpaint(dit, \n",
    "                                            noise_input, \n",
    "                                            total_steps=timesteps,\n",
    "                                            target=latents,\n",
    "                                            mask=1 - face_mask_tensor)\n",
    "        \n",
    "        fake_sample = vae.decode(fake_latents / 0.18215).sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (5, 5))\n",
    "comb_samples = torch.cat((test_tensor, fake_sample.detach()), 0)\n",
    "out = vutils.make_grid(comb_samples.detach().float().cpu(), nrow=3, normalize=True)\n",
    "_ = plt.imshow(out.numpy().transpose((1, 2, 0)))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
