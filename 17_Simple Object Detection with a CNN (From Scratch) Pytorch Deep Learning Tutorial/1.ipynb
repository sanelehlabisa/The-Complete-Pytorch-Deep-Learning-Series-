{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN Object Detection in Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sanele Hlabisa\\Desktop\\2025\\The Complete Pytorch Deep Learning Series!\\venv\\Lib\\site-packages\\albumentations\\check_version.py:51: UserWarning: Error fetching version info <urlopen error _ssl.c:983: The handshake operation timed out>\n",
      "  data = fetch_version_info()\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.models as models\n",
    "\n",
    "# Lets install a new package called albumentations\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import trange, tqdm\n",
    "from PIL import Image, ImageOps\n",
    "import copy\n",
    "import pandas as pd\n",
    "\n",
    "from Trainer import ModelTrainer\n",
    "\n",
    "from Datasets import CUB200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the size of our mini batch\n",
    "batch_size = 16\n",
    "\n",
    "# Number of iterations\n",
    "num_epochs = 64\n",
    "\n",
    "# Optimizer learning rate\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# Download dataset from https://www.kaggle.com/datasets/wenewone/cub2002011\n",
    "# Then unzip the folder into the datasets subfolder\n",
    "dataset_root_folder = \"../datasets/cub_200\"\n",
    "\n",
    "# Set image size\n",
    "image_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training and saving trick\n",
    "start_from_checkpoints = False\n",
    "save_dir = \"../data/Models\"\n",
    "model_name = \"ResNet34_CUB\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device to GPU if cuda is available else set it to cpu\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sanele Hlabisa\\Desktop\\2025\\The Complete Pytorch Deep Learning Series!\\venv\\Lib\\site-packages\\pydantic\\main.py:426: UserWarning: Pydantic serializer warnings:\n",
      "  Expected `dict[str, any]` but got `UniformParams` with value `UniformParams(noise_type=..., 0.09803921568627451)])` - serialized value may not be as expected\n",
      "  return self.__pydantic_serializer__.to_python(\n"
     ]
    }
   ],
   "source": [
    "# Data Augmentaions using Albumentation library\n",
    "train_transform = A.Compose([\n",
    "    A.SmallestMaxSize(max_size=image_size),\n",
    "    A.RandomCrop(height=image_size, width=image_size),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.ShiftScaleRotate(shift_limit=0.2, scale_limit=0.2, rotate_limit=30, p=0.5),\n",
    "    A.RGBShift(r_shift_limit=25, g_shift_limit=25, b_shift_limit=25, p=0.5),\n",
    "    A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2()\n",
    "],\n",
    "bbox_params=A.BboxParams(format=\"coco\", min_area=0, min_visibility=0, label_fields=['class_labels']))\n",
    "\n",
    "transform = A.Compose([\n",
    "    A.SmallestMaxSize(max_size=image_size),\n",
    "    A.RandomCrop(height=image_size, width=image_size),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2()\n",
    "],\n",
    "bbox_params=A.BboxParams(format=\"coco\", min_area=0, min_visibility=0, label_fields=['class_labels'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To train our model we will use logistic regression and to measure the performance of our model we will use Intersection over Union (IoU)\n",
    "\n",
    "# nn.Module class that will return the IoU for a batch of outputs\n",
    "class BboxIOU(nn.Module):\n",
    "    \n",
    "    def xyhw_to_xyxy(self, bbox):\n",
    "        \"\"\"\n",
    "        Converts from (x_min, y_min, width, height) to (x_min, y_min, x_max, y_max) format\n",
    "        \"\"\"\n",
    "        new_bbox = torch.cat((bbox[:, 0:1], \n",
    "                              bbox[:, 1:2],\n",
    "                              bbox[:, 2:3] + bbox[:, 0:1], \n",
    "                              bbox[:, 3:4] + bbox[:, 1:2]), 1)\n",
    "        return new_bbox\n",
    "\n",
    "    def bb_intersection_over_union(self, pred_xyhw, target_xyhw):\n",
    "        pred_xyxy = self.xyhw_to_xyxy(pred_xyhw)\n",
    "        target_xyxy = self.xyhw_to_xyxy(target_xyhw)\n",
    "\n",
    "        # Determine the (x, y)-coordinates of the intersection rectangle\n",
    "        xA = torch.cat((pred_xyxy[:, 0:1], target_xyxy[:, 0:1]), 1).max(dim=1)[0].unsqueeze(1)\n",
    "        yA = torch.cat((pred_xyxy[:, 1:2], target_xyxy[:, 1:2]), 1).max(dim=1)[0].unsqueeze(1)\n",
    "        xB = torch.cat((pred_xyxy[:, 2:3], target_xyxy[:, 2:3]), 1).min(dim=1)[0].unsqueeze(1)\n",
    "        yB = torch.cat((pred_xyxy[:, 3:4], target_xyxy[:, 3:4]), 1).min(dim=1)[0].unsqueeze(1)\n",
    "\n",
    "        # Compute the area of intersection rectangle\n",
    "        x_len = F.relu(xB - xA)\n",
    "        y_len = F.relu(yB - yA)\n",
    "        # Negative area means no overlap\n",
    "        interArea = x_len * y_len\n",
    "\n",
    "        # If you don't have xyhw values, calculate areas like this\n",
    "#         w1 = (pred_xyxy[:, 0:1] - pred_xyxy[:, 2:3]).abs()\n",
    "#         h1 = (pred_xyxy[:, 1:2] - pred_xyxy[:, 3:4]).abs()\n",
    "\n",
    "#         w2 = (target_xyxy[:, 0:1] - target_xyxy[:, 2:3]).abs()\n",
    "#         h2 = (target_xyxy[:, 1:2] - target_xyxy[:, 3:4]).abs()\n",
    "\n",
    "#         area1 = w1 * h1\n",
    "#         area2 = w2 * h2\n",
    "\n",
    "        area1 = pred_xyhw[:, 2:3] * pred_xyhw[:, 3:4]\n",
    "        area2 = target_xyhw[:, 2:3] * target_xyhw[:, 3:4]\n",
    "\n",
    "        # Compute the intersection over union by taking the intersection\n",
    "        # area and dividing it by the sum of prediction + ground-truth\n",
    "        # areas - the interesection area\n",
    "        iou = interArea / (area1 + area2 - interArea + 1e-5)\n",
    "\n",
    "        # Return the intersection over union value\n",
    "        return iou\n",
    "\n",
    "    def forward(self, predictions, data):\n",
    "        \"\"\"\n",
    "        data: list of data, index 0 is the input image index [0] is the target\n",
    "        predictions: raw output of the model, the first 4 outputs are assumed to be the bounding box values\n",
    "        \"\"\"\n",
    "        \n",
    "        pred_bbox = torch.sigmoid(predictions[:, :4])\n",
    "        target_bbox = data[1].to(pred_bbox.device)\n",
    "        \n",
    "        return self.bb_intersection_over_union(pred_bbox, target_bbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our Datasets\n",
    "# You'll need to download the dataset from Kaggle\n",
    "# https://www.kaggle.com/datasets/wenewone/cub2002011\n",
    "# Unzip it (and the directories it contains) into the datasets directory \n",
    "# and rename the top-level directory cub_200\n",
    "\n",
    "train_data = CUB200(dataset_root_folder, image_size=image_size, transform=train_transform, test_train=0)\n",
    "\n",
    "test_data = CUB200(dataset_root_folder, image_size=image_size, transform=transform, test_train=1)\n",
    "\n",
    "# Split training data into train and validation with 0.9:0.1 ratio\n",
    "validation_split = 0.9\n",
    "\n",
    "n_train_examples = int(len(train_data) * validation_split)\n",
    "n_valid_examples = len(train_data) - n_train_examples\n",
    "train_data, valid_data = torch.utils.data.random_split(train_data, [n_train_examples, n_valid_examples], generator=torch.Generator().manual_seed(42))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the ResNet34 Model\n",
    "# res_net = models.resnet34(pretrained=True)\n",
    "# or\n",
    "res_net = models.resnet34(weights=\"IMAGENET1K_V1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting from scratch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sanele Hlabisa\\Desktop\\2025\\The Complete Pytorch Deep Learning Series!\\17_Simple Object Detection with a CNN (From Scratch) Pytorch Deep Learning Tutorial\\Trainer.py:37: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler()\n",
      "c:\\Users\\Sanele Hlabisa\\Desktop\\2025\\The Complete Pytorch Deep Learning Series!\\venv\\Lib\\site-packages\\torch\\amp\\grad_scaler.py:132: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_trainer = ModelTrainer(\n",
    "    model=res_net.to(device), \n",
    "    output_size=4, \n",
    "    device=device, \n",
    "    loss_fun=nn.BCEWithLogitsLoss(), \n",
    "    batch_size=batch_size, \n",
    "    learning_rate=learning_rate, \n",
    "    save_dir=save_dir, \n",
    "    model_name=model_name, \n",
    "    eval_metric=BboxIOU(), \n",
    "    start_from_checkpoint=start_from_checkpoints\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 7376\n",
      "Number of validation examples: 820\n",
      "Number of testing examples: 3592\n"
     ]
    }
   ],
   "source": [
    "model_trainer.set_data(\n",
    "    train_set=train_data,\n",
    "    test_set=test_data,\n",
    "    val_set=valid_data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_trainer.set_lr_schedule(optim.lr_scheduler.StepLR(model_trainer.optimizer, step_size=1, gamma=0.95))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error occurred while fetching data from DataLoader: Caught TypeError in DataLoader worker process 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Sanele Hlabisa\\Desktop\\2025\\The Complete Pytorch Deep Learning Series!\\venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\worker.py\", line 351, in _worker_loop\n",
      "    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Sanele Hlabisa\\Desktop\\2025\\The Complete Pytorch Deep Learning Series!\\venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 55, in fetch\n",
      "    return self.collate_fn(data)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Sanele Hlabisa\\Desktop\\2025\\The Complete Pytorch Deep Learning Series!\\venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 398, in default_collate\n",
      "    return collate(batch, collate_fn_map=default_collate_fn_map)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Sanele Hlabisa\\Desktop\\2025\\The Complete Pytorch Deep Learning Series!\\venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 240, in collate\n",
      "    raise TypeError(default_collate_err_msg_format.format(elem_type))\n",
      "TypeError: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'NoneType'>\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 2000x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "# Attempt to get the next batch from the DataLoader\n",
    "try:\n",
    "    images, bbox, labels = next(iter(model_trainer.test_loader))\n",
    "    out = torchvision.utils.make_grid(images, normalize=True)\n",
    "    _ = plt.imshow(out.numpy().transpose((1, 2, 0)))\n",
    "except Exception as e:\n",
    "    print(\"Error occurred while fetching data from DataLoader:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'images' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m example_indx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[1;32m----> 2\u001b[0m ex_img \u001b[38;5;241m=\u001b[39m \u001b[43mimages\u001b[49m[example_indx]\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# The bounding box is represented in the (x_min, y_min, width, height) format\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# aka the coordinate of the top left corner of the box and the box height and width\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# draw_bounding_boxes expects it in the (x_min, y_min, x_max, y_max) formatweights=ResNet18_Weights.IMAGENET1K_V1\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# aka the coordinates of the top left and bottom right corners of the box\u001b[39;00m\n\u001b[0;32m      9\u001b[0m ex_label \u001b[38;5;241m=\u001b[39m bbox[example_indx]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m*\u001b[39m image_size\n",
      "\u001b[1;31mNameError\u001b[0m: name 'images' is not defined"
     ]
    }
   ],
   "source": [
    "example_indx = 3\n",
    "ex_img = images[example_indx]\n",
    "\n",
    "# The bounding box is represented in the (x_min, y_min, width, height) format\n",
    "# aka the coordinate of the top left corner of the box and the box height and width\n",
    "\n",
    "# draw_bounding_boxes expects it in the (x_min, y_min, x_max, y_max) formatweights=ResNet18_Weights.IMAGENET1K_V1\n",
    "# aka the coordinates of the top left and bottom right corners of the box\n",
    "ex_label = bbox[example_indx].unsqueeze(0) * image_size\n",
    "ex_label[:, 2] += ex_label[:, 0]\n",
    "ex_label[:, 3] += ex_label[:, 1]\n",
    "\n",
    "img_out = (((ex_img - ex_img.min())/(ex_img.max() - ex_img.min())) * 255).to(torch.uint8)\n",
    "img_box = torchvision.utils.draw_bounding_boxes(img_out, ex_label, colors=(0, 255, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (5,5))\n",
    "out = torchvision.utils.make_grid(img_box.unsqueeze(0).float(), normalize=True)\n",
    "_ = plt.imshow(out.numpy().transpose((1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets see how many Parameter's our Model has!\n",
    "num_params = 0\n",
    "for param in model_trainer.model.parameters():\n",
    "    num_params += param.flatten().shape[0]\n",
    "print(\"This model has %d (approximately %d Million) Parameters!\" % (num_params, num_params//1e6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_trainer.run_training(num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The highest validation IoU was %.2f\" %(model_trainer.best_valid_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.figure(figsize = (10,5))\n",
    "train_x = np.linspace(0, num_epochs, len(model_trainer.train_loss_logger))\n",
    "_ = plt.plot(train_x, model_trainer.train_loss_logger)\n",
    "_ = plt.title(\"Training Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select an image to test\n",
    "example_indx = 50\n",
    "ex_img = images[example_indx]\n",
    "img_out = (((ex_img - ex_img.min())/(ex_img.max() - ex_img.min())) * 255).to(torch.uint8)\n",
    "\n",
    "real_label = bbox[example_indx].unsqueeze(0) * image_size\n",
    "real_label[:, 2] += real_label[:, 0]\n",
    "real_label[:, 3] += real_label[:, 1]\n",
    "\n",
    "# Get the model's prediction for the Bounding Box\n",
    "model_trainer.eval()\n",
    "with torch.no_grad():\n",
    "    pred_out = torch.sigmoid(model_trainer(ex_img.unsqueeze(0).to(device)))\n",
    "    pred_label = (pred_out * image_size).cpu()\n",
    "    pred_label[:, 2] += pred_label[:, 0]\n",
    "    pred_label[:, 3] += pred_label[:, 1]\n",
    "    \n",
    "# Draw the box on the image\n",
    "img_box = torchvision.utils.draw_bounding_boxes(img_out, real_label, colors=(0, 255, 0))\n",
    "img_box = torchvision.utils.draw_bounding_boxes(img_box, pred_label, colors=(255, 0, 0))\n",
    "\n",
    "plt.figure(figsize = (5,5))\n",
    "out = torchvision.utils.make_grid(img_box.unsqueeze(0).float(), normalize=True)\n",
    "_ = plt.imshow(out.numpy().transpose((1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.figure(figsize = (10,5))\n",
    "train_x = np.linspace(0, num_epochs, len(model_trainer.train_acc_logger))\n",
    "_ = plt.plot(train_x, model_trainer.train_acc_logger, c = \"y\")\n",
    "valid_x = np.linspace(0, num_epochs, len(model_trainer.val_acc_logger))\n",
    "_ = plt.plot(valid_x, model_trainer.val_acc_logger, c = \"k\")\n",
    "\n",
    "_ = plt.title(\"Average IoU\")\n",
    "_ = plt.legend([\"Training IoU\", \"Validation IoU\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the evaluate function and pass the evaluation/test dataloader etc\n",
    "test_acc = model_trainer.evaluate_model(train_test_val=\"test\")\n",
    "print(\"The Test Average IoU is: %.2f\" %(test_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
