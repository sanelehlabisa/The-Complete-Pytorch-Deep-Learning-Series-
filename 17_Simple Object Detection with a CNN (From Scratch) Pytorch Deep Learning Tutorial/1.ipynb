{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN Object Detection in Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.models as models\n",
    "\n",
    "# Lets install a new package called albumentations\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import trange, tqdm\n",
    "from PIL import Image, ImageOps\n",
    "import copy\n",
    "import pandas as pd\n",
    "\n",
    "from Trainer import ModelTrainer\n",
    "\n",
    "from Datasets import CUB200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the size of our mini batch\n",
    "batch_size = 16\n",
    "\n",
    "# Number of iterations\n",
    "num_epochs = 64\n",
    "\n",
    "# Optimizer learning rate\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# Download dataset from https://www.kaggle.com/datasets/wenewone/cub2002011\n",
    "# Then unzip the folder into the datasets subfolder\n",
    "dataset_root_folder = \"../datasets/cub_200\"\n",
    "\n",
    "# Set image size\n",
    "image_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training and saving trick\n",
    "start_from_checkpoints = False\n",
    "save_dir = \"../data/Models\"\n",
    "model_name = \"ResNet34_CUB\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device to GPU if cuda is available else set it to cpu\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sanele Hlabisa\\Desktop\\2025\\The Complete Pytorch Deep Learning Series!\\venv\\Lib\\site-packages\\pydantic\\main.py:426: UserWarning: Pydantic serializer warnings:\n",
      "  Expected `dict[str, any]` but got `UniformParams` with value `UniformParams(noise_type=..., 0.09803921568627451)])` - serialized value may not be as expected\n",
      "  return self.__pydantic_serializer__.to_python(\n"
     ]
    }
   ],
   "source": [
    "# Data Augmentaions using Albumentation library\n",
    "train_transform = A.Compose([\n",
    "    A.SmallestMaxSize(max_size=image_size),\n",
    "    A.RandomCrop(height=image_size, width=image_size),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.ShiftScaleRotate(shift_limit=0.2, scale_limit=0.2, rotate_limit=30, p=0.5),\n",
    "    A.RGBShift(r_shift_limit=25, g_shift_limit=25, b_shift_limit=25, p=0.5),\n",
    "    A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2()\n",
    "],\n",
    "bbox_params=A.BboxParams(format=\"coco\", min_area=0, min_visibility=0, label_fields=['class_labels']))\n",
    "\n",
    "transform = A.Compose([\n",
    "    A.SmallestMaxSize(max_size=image_size),\n",
    "    A.RandomCrop(height=image_size, width=image_size),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2()\n",
    "],\n",
    "bbox_params=A.BboxParams(format=\"coco\", min_area=0, min_visibility=0, label_fields=['class_labels'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To train our model we will use logistic regression and to measure the performance of our model we will use Intersection over Union (IoU)\n",
    "\n",
    "# nn.Module class that will return the IoU for a batch of outputs\n",
    "class BboxIOU(nn.Module):\n",
    "    \n",
    "    def xyhw_to_xyxy(self, bbox):\n",
    "        \"\"\"\n",
    "        Converts from (x_min, y_min, width, height) to (x_min, y_min, x_max, y_max) format\n",
    "        \"\"\"\n",
    "        new_bbox = torch.cat((bbox[:, 0:1], \n",
    "                              bbox[:, 1:2],\n",
    "                              bbox[:, 2:3] + bbox[:, 0:1], \n",
    "                              bbox[:, 3:4] + bbox[:, 1:2]), 1)\n",
    "        return new_bbox\n",
    "\n",
    "    def bb_intersection_over_union(self, pred_xyhw, target_xyhw):\n",
    "        pred_xyxy = self.xyhw_to_xyxy(pred_xyhw)\n",
    "        target_xyxy = self.xyhw_to_xyxy(target_xyhw)\n",
    "\n",
    "        # Determine the (x, y)-coordinates of the intersection rectangle\n",
    "        xA = torch.cat((pred_xyxy[:, 0:1], target_xyxy[:, 0:1]), 1).max(dim=1)[0].unsqueeze(1)\n",
    "        yA = torch.cat((pred_xyxy[:, 1:2], target_xyxy[:, 1:2]), 1).max(dim=1)[0].unsqueeze(1)\n",
    "        xB = torch.cat((pred_xyxy[:, 2:3], target_xyxy[:, 2:3]), 1).min(dim=1)[0].unsqueeze(1)\n",
    "        yB = torch.cat((pred_xyxy[:, 3:4], target_xyxy[:, 3:4]), 1).min(dim=1)[0].unsqueeze(1)\n",
    "\n",
    "        # Compute the area of intersection rectangle\n",
    "        x_len = F.relu(xB - xA)\n",
    "        y_len = F.relu(yB - yA)\n",
    "        # Negative area means no overlap\n",
    "        interArea = x_len * y_len\n",
    "\n",
    "        # If you don't have xyhw values, calculate areas like this\n",
    "#         w1 = (pred_xyxy[:, 0:1] - pred_xyxy[:, 2:3]).abs()\n",
    "#         h1 = (pred_xyxy[:, 1:2] - pred_xyxy[:, 3:4]).abs()\n",
    "\n",
    "#         w2 = (target_xyxy[:, 0:1] - target_xyxy[:, 2:3]).abs()\n",
    "#         h2 = (target_xyxy[:, 1:2] - target_xyxy[:, 3:4]).abs()\n",
    "\n",
    "#         area1 = w1 * h1\n",
    "#         area2 = w2 * h2\n",
    "\n",
    "        area1 = pred_xyhw[:, 2:3] * pred_xyhw[:, 3:4]\n",
    "        area2 = target_xyhw[:, 2:3] * target_xyhw[:, 3:4]\n",
    "\n",
    "        # Compute the intersection over union by taking the intersection\n",
    "        # area and dividing it by the sum of prediction + ground-truth\n",
    "        # areas - the interesection area\n",
    "        iou = interArea / (area1 + area2 - interArea + 1e-5)\n",
    "\n",
    "        # Return the intersection over union value\n",
    "        return iou\n",
    "\n",
    "    def forward(self, predictions, data):\n",
    "        \"\"\"\n",
    "        data: list of data, index 0 is the input image index [0] is the target\n",
    "        predictions: raw output of the model, the first 4 outputs are assumed to be the bounding box values\n",
    "        \"\"\"\n",
    "        \n",
    "        pred_bbox = torch.sigmoid(predictions[:, :4])\n",
    "        target_bbox = data[1].to(pred_bbox.device)\n",
    "        \n",
    "        return self.bb_intersection_over_union(pred_bbox, target_bbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our Datasets\n",
    "# You'll need to download the dataset from Kaggle\n",
    "# https://www.kaggle.com/datasets/wenewone/cub2002011\n",
    "# Unzip it (and the directories it contains) into the datasets directory \n",
    "# and rename the top-level directory cub_200\n",
    "\n",
    "train_data = CUB200(dataset_root_folder, image_size=image_size, transform=train_transform, test_train=0)\n",
    "\n",
    "test_data = CUB200(dataset_root_folder, image_size=image_size, transform=transform, test_train=1)\n",
    "\n",
    "# Split training data into train and validation with 0.9:0.1 ratio\n",
    "validation_split = 0.9\n",
    "\n",
    "n_train_examples = int(len(train_data) * validation_split)\n",
    "n_valid_examples = len(train_data) - n_train_examples\n",
    "train_data, valid_data = torch.utils.data.random_split(train_data, [n_train_examples, n_valid_examples], generator=torch.Generator().manual_seed(42))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the ResNet34 Model\n",
    "# res_net = models.resnet34(pretrained=True)\n",
    "# or\n",
    "res_net = models.resnet34(weights=\"IMAGENET1K_V1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting from scratch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sanele Hlabisa\\Desktop\\2025\\The Complete Pytorch Deep Learning Series!\\17_Simple Object Detection with a CNN (From Scratch) Pytorch Deep Learning Tutorial\\Trainer.py:37: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler()\n",
      "c:\\Users\\Sanele Hlabisa\\Desktop\\2025\\The Complete Pytorch Deep Learning Series!\\venv\\Lib\\site-packages\\torch\\amp\\grad_scaler.py:132: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_trainer = ModelTrainer(\n",
    "    model=res_net.to(device), \n",
    "    output_size=4, \n",
    "    device=device, \n",
    "    loss_fun=nn.BCEWithLogitsLoss(), \n",
    "    batch_size=batch_size, \n",
    "    learning_rate=learning_rate, \n",
    "    save_dir=save_dir, \n",
    "    model_name=model_name, \n",
    "    eval_metric=BboxIOU(), \n",
    "    start_from_checkpoint=start_from_checkpoints\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 7376\n",
      "Number of validation examples: 820\n",
      "Number of testing examples: 3592\n"
     ]
    }
   ],
   "source": [
    "model_trainer.set_data(\n",
    "    train_set=train_data,\n",
    "    test_set=test_data,\n",
    "    val_set=valid_data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_trainer.set_lr_schedule(optim.lr_scheduler.StepLR(model_trainer.optimizer, step_size=1, gamma=0.95))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "Caught error in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"c:\\Users\\Sanele Hlabisa\\Desktop\\2025\\The Complete Pytorch Deep Learning Series!\\venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\worker.py\", line 351, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Sanele Hlabisa\\Desktop\\2025\\The Complete Pytorch Deep Learning Series!\\venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"c:\\Users\\Sanele Hlabisa\\Desktop\\2025\\The Complete Pytorch Deep Learning Series!\\17_Simple Object Detection with a CNN (From Scratch) Pytorch Deep Learning Tutorial\\Datasets.py\", line 72, in __getitem__\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\ncv2.error: OpenCV(4.10.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\color.cpp:196: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m20\u001b[39m,\u001b[38;5;241m10\u001b[39m))\n\u001b[1;32m----> 2\u001b[0m images, bbox, labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m out \u001b[38;5;241m=\u001b[39m torchvision\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mmake_grid(images, normalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      4\u001b[0m _ \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39mimshow(out\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mtranspose((\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m)))\n",
      "File \u001b[1;32mc:\\Users\\Sanele Hlabisa\\Desktop\\2025\\The Complete Pytorch Deep Learning Series!\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\Sanele Hlabisa\\Desktop\\2025\\The Complete Pytorch Deep Learning Series!\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1465\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1463\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1464\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[1;32m-> 1465\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Sanele Hlabisa\\Desktop\\2025\\The Complete Pytorch Deep Learning Series!\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1491\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m   1489\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[0;32m   1490\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[1;32m-> 1491\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1492\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\Sanele Hlabisa\\Desktop\\2025\\The Complete Pytorch Deep Learning Series!\\venv\\Lib\\site-packages\\torch\\_utils.py:715\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    711\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m    712\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[0;32m    713\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[0;32m    714\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 715\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[1;31merror\u001b[0m: Caught error in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"c:\\Users\\Sanele Hlabisa\\Desktop\\2025\\The Complete Pytorch Deep Learning Series!\\venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\worker.py\", line 351, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Sanele Hlabisa\\Desktop\\2025\\The Complete Pytorch Deep Learning Series!\\venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"c:\\Users\\Sanele Hlabisa\\Desktop\\2025\\The Complete Pytorch Deep Learning Series!\\17_Simple Object Detection with a CNN (From Scratch) Pytorch Deep Learning Tutorial\\Datasets.py\", line 72, in __getitem__\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\ncv2.error: OpenCV(4.10.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\color.cpp:196: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 2000x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize = (20,10))\n",
    "images, bbox, labels = iter(model_trainer.test_loader)._next_data[0]\n",
    "out = torchvision.utils.make_grid(images, normalize=True)\n",
    "_ = plt.imshow(out.numpy().transpose((1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_indx = 3\n",
    "ex_img = images[example_indx]\n",
    "\n",
    "# The bounding box is represented in the (x_min, y_min, width, height) format\n",
    "# aka the coordinate of the top left corner of the box and the box height and width\n",
    "\n",
    "# draw_bounding_boxes expects it in the (x_min, y_min, x_max, y_max) formatweights=ResNet18_Weights.IMAGENET1K_V1\n",
    "# aka the coordinates of the top left and bottom right corners of the box\n",
    "ex_label = bbox[example_indx].unsqueeze(0) * image_size\n",
    "ex_label[:, 2] += ex_label[:, 0]\n",
    "ex_label[:, 3] += ex_label[:, 1]\n",
    "\n",
    "img_out = (((ex_img - ex_img.min())/(ex_img.max() - ex_img.min())) * 255).to(torch.uint8)\n",
    "img_box = torchvision.utils.draw_bounding_boxes(img_out, ex_label, colors=(0, 255, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (5,5))\n",
    "out = torchvision.utils.make_grid(img_box.unsqueeze(0).float(), normalize=True)\n",
    "_ = plt.imshow(out.numpy().transpose((1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets see how many Parameter's our Model has!\n",
    "num_params = 0\n",
    "for param in model_trainer.model.parameters():\n",
    "    num_params += param.flatten().shape[0]\n",
    "print(\"This model has %d (approximately %d Million) Parameters!\" % (num_params, num_params//1e6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_trainer.run_training(num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The highest validation IoU was %.2f\" %(model_trainer.best_valid_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.figure(figsize = (10,5))\n",
    "train_x = np.linspace(0, num_epochs, len(model_trainer.train_loss_logger))\n",
    "_ = plt.plot(train_x, model_trainer.train_loss_logger)\n",
    "_ = plt.title(\"Training Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select an image to test\n",
    "example_indx = 50\n",
    "ex_img = images[example_indx]\n",
    "img_out = (((ex_img - ex_img.min())/(ex_img.max() - ex_img.min())) * 255).to(torch.uint8)\n",
    "\n",
    "real_label = bbox[example_indx].unsqueeze(0) * image_size\n",
    "real_label[:, 2] += real_label[:, 0]\n",
    "real_label[:, 3] += real_label[:, 1]\n",
    "\n",
    "# Get the model's prediction for the Bounding Box\n",
    "model_trainer.eval()\n",
    "with torch.no_grad():\n",
    "    pred_out = torch.sigmoid(model_trainer(ex_img.unsqueeze(0).to(device)))\n",
    "    pred_label = (pred_out * image_size).cpu()\n",
    "    pred_label[:, 2] += pred_label[:, 0]\n",
    "    pred_label[:, 3] += pred_label[:, 1]\n",
    "    \n",
    "# Draw the box on the image\n",
    "img_box = torchvision.utils.draw_bounding_boxes(img_out, real_label, colors=(0, 255, 0))\n",
    "img_box = torchvision.utils.draw_bounding_boxes(img_box, pred_label, colors=(255, 0, 0))\n",
    "\n",
    "plt.figure(figsize = (5,5))\n",
    "out = torchvision.utils.make_grid(img_box.unsqueeze(0).float(), normalize=True)\n",
    "_ = plt.imshow(out.numpy().transpose((1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.figure(figsize = (10,5))\n",
    "train_x = np.linspace(0, num_epochs, len(model_trainer.train_acc_logger))\n",
    "_ = plt.plot(train_x, model_trainer.train_acc_logger, c = \"y\")\n",
    "valid_x = np.linspace(0, num_epochs, len(model_trainer.val_acc_logger))\n",
    "_ = plt.plot(valid_x, model_trainer.val_acc_logger, c = \"k\")\n",
    "\n",
    "_ = plt.title(\"Average IoU\")\n",
    "_ = plt.legend([\"Training IoU\", \"Validation IoU\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the evaluate function and pass the evaluation/test dataloader etc\n",
    "test_acc = model_trainer.evaluate_model(train_test_val=\"test\")\n",
    "print(\"The Test Average IoU is: %.2f\" %(test_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
