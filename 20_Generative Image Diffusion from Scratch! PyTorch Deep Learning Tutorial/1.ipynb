{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deffusion Model in Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import imageio\n",
    "import copy\n",
    "import math\n",
    "\n",
    "from tqdm import trange, tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training parameters\n",
    "batch_size = 64\n",
    "lr = 1e-4\n",
    "\n",
    "train_epoch = 16\n",
    "\n",
    "# data_loader\n",
    "img_size = 32\n",
    "\n",
    "data_set_root = \"../datasets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "gpu_indx  = 0\n",
    "device = torch.device(gpu_indx if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "                                transforms.Resize(img_size),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean=([0.5]), std=([0.5]))\n",
    "                                ])\n",
    "\n",
    "trainset = datasets.FashionMNIST(data_set_root, train=True, download=True, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalNorm2d(nn.Module):\n",
    "    def __init__(self, channels, num_features, norm_type=\"gn\"):\n",
    "        super(ConditionalNorm2d, self).__init__()\n",
    "        self.channels = channels\n",
    "        if norm_type == \"bn\":\n",
    "            self.norm = nn.BatchNorm2d(channels, affine=False, eps=1e-4)\n",
    "        elif norm_type == \"gn\":\n",
    "            self.norm = nn.GroupNorm(8, channels, affine=False, eps=1e-4)\n",
    "        else:\n",
    "            raise ValueError(\"Normalisation type not recognised.\")\n",
    "\n",
    "        self.fcw = nn.Linear(num_features, channels)\n",
    "        self.fcb = nn.Linear(num_features, channels)\n",
    "\n",
    "    def forward(self, x, features):\n",
    "        out = self.norm(x)\n",
    "        w = self.fcw(features)\n",
    "        b = self.fcb(features)\n",
    "\n",
    "        out = w.view(-1, self.channels, 1, 1) * out + b.view(-1, self.channels, 1, 1)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResDown(nn.Module):\n",
    "    \"\"\"\n",
    "    Residual down sampling block\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, channel_in, channel_out, kernel_size=3, num_features=128):\n",
    "        super(ResDown, self).__init__()\n",
    "        self.norm1 = ConditionalNorm2d(channel_in, num_features=num_features)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(channel_in, channel_out, kernel_size, 2, kernel_size // 2)\n",
    "        self.norm2 = ConditionalNorm2d(channel_out, num_features=num_features)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(channel_out, channel_out, kernel_size, 1, kernel_size // 2)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(channel_in, channel_out, kernel_size, 2, kernel_size // 2)\n",
    "\n",
    "        self.act_fnc = nn.ELU()\n",
    "\n",
    "    def forward(self, x, features):\n",
    "        x = self.act_fnc(self.norm1(x, features))\n",
    "\n",
    "        skip = self.conv3(x)\n",
    "\n",
    "        x = self.act_fnc(self.norm2(self.conv1(x), features))\n",
    "        x = self.conv2(x)\n",
    "\n",
    "        return x + skip\n",
    "\n",
    "\n",
    "class ResUp(nn.Module):\n",
    "    \"\"\"\n",
    "    Residual up sampling block\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, channel_in, channel_out, kernel_size=3, scale_factor=2, num_features=128):\n",
    "        super(ResUp, self).__init__()\n",
    "        self.norm1 = ConditionalNorm2d(channel_in, num_features=num_features)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(channel_in, channel_in, kernel_size, 1, kernel_size // 2)\n",
    "        self.norm2 = ConditionalNorm2d(channel_in, num_features=num_features)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(channel_in, channel_out, kernel_size, 1, kernel_size // 2)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(channel_in, channel_out, kernel_size, 1, kernel_size // 2)\n",
    "\n",
    "        self.up_nn = nn.Upsample(scale_factor=scale_factor, mode=\"nearest\")\n",
    "\n",
    "        self.act_fnc = nn.ELU()\n",
    "\n",
    "    def forward(self, x, features):\n",
    "        x = self.up_nn(self.act_fnc(self.norm1(x, features)))\n",
    "\n",
    "        skip = self.conv3(x)\n",
    "\n",
    "        x = self.act_fnc(self.norm2(self.conv1(x), features))\n",
    "        x = self.conv2(x)\n",
    "\n",
    "        return x + skip\n",
    "\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Residual block\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, channel_in, channel_out, kernel_size=3, num_features=128):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.norm1 = ConditionalNorm2d(channel_in, num_features=num_features)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(channel_in, channel_in, kernel_size, 1, kernel_size // 2)\n",
    "        self.norm2 = ConditionalNorm2d(channel_in, num_features=num_features)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(channel_in, channel_out, kernel_size, 1, kernel_size // 2)\n",
    "\n",
    "        if not channel_in == channel_out:\n",
    "            self.conv3 = nn.Conv2d(channel_in, channel_out, kernel_size, 1, kernel_size // 2)\n",
    "        else:\n",
    "            self.conv3 = None\n",
    "\n",
    "        self.act_fnc = nn.ELU()\n",
    "\n",
    "    def forward(self, x_in, features):\n",
    "        x = self.act_fnc(self.norm1(x_in, features))\n",
    "\n",
    "        if self.conv3 is not None:\n",
    "            skip = self.conv3(x)\n",
    "        else:\n",
    "            skip = x_in\n",
    "\n",
    "        x = self.act_fnc(self.norm2(self.conv1(x), features))\n",
    "        x = self.conv2(x)\n",
    "\n",
    "        return x + skip\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder block\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, channels, ch=64, blocks=(1, 2, 4), num_features=128):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv_in = nn.Conv2d(channels, blocks[0] * ch, 3, 1, 1)\n",
    "\n",
    "        widths_in = list(blocks)\n",
    "        widths_out = list(blocks[1:]) + [blocks[-1]]\n",
    "\n",
    "        self.layer_blocks = nn.ModuleList([])\n",
    "\n",
    "        for w_in, w_out in zip(widths_in, widths_out):\n",
    "            self.layer_blocks.append(ResDown(w_in * ch, w_out * ch, num_features=num_features))\n",
    "\n",
    "        self.block_out = ResBlock(w_out * ch, w_out * ch, num_features=num_features)\n",
    "\n",
    "        self.act_fnc = nn.ELU()\n",
    "\n",
    "    def forward(self, x, index_features):\n",
    "        x = self.conv_in(x)\n",
    "        skip_list = [x]\n",
    "\n",
    "        for block in self.layer_blocks:\n",
    "            x = block(x, index_features)\n",
    "            skip_list.append(x)\n",
    "        \n",
    "        x = self.block_out(x, index_features)\n",
    "        return x, skip_list\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder block\n",
    "    Built to be a mirror of the encoder block\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, channels, ch=64, blocks=(1, 2, 4), num_features=128):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        widths_out = list(blocks)[::-1]\n",
    "        widths_in = (list(blocks[1:]) + [blocks[-1]])[::-1]\n",
    "\n",
    "        self.block_in = ResBlock(blocks[-1] * ch, blocks[-1] * ch, num_features=num_features)\n",
    "        \n",
    "        self.layer_blocks = nn.ModuleList([])\n",
    "\n",
    "        for w_in, w_out in zip(widths_in, widths_out):\n",
    "            self.layer_blocks.append(ResUp(w_in * ch * 2, w_out * ch, num_features=num_features))\n",
    "\n",
    "        self.conv_out = nn.Conv2d(blocks[0] * ch * 2, channels, 3, 1, 1)\n",
    "        self.act_fnc = nn.ELU()\n",
    "\n",
    "    def forward(self, x_in, skip_list, index_features):\n",
    "        x = self.block_in(x_in, index_features)\n",
    "        \n",
    "        for block in self.layer_blocks:\n",
    "            skip = skip_list.pop()\n",
    "            x = torch.cat((x, skip), 1)\n",
    "            x = block(x, index_features)\n",
    "            \n",
    "        skip = skip_list.pop()\n",
    "        x = torch.cat((x, skip), 1)\n",
    "        return self.conv_out(x)\n",
    "\n",
    "    \n",
    "class SinusoidalPosEmb(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        device = x.device\n",
    "        half_dim = self.dim // 2\n",
    "        emb = math.log(10000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
    "        emb = x[:, None] * emb[None, :]\n",
    "        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
    "        return emb\n",
    "\n",
    "\n",
    "class Unet(nn.Module):\n",
    "    \"\"\"\n",
    "    Unet network, uses the above encoder and decoder blocks\n",
    "    \"\"\"\n",
    "    def __init__(self, channel_in=3, ch=32, blocks=(1, 2, 4), timesteps=20, num_features=128):\n",
    "        super(Unet, self).__init__()        \n",
    "        self.time_mlp = nn.Sequential(\n",
    "            SinusoidalPosEmb(num_features),\n",
    "            nn.Linear(num_features, 2 * num_features),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(2 * num_features, num_features),\n",
    "            nn.GELU()\n",
    "        )\n",
    "        \n",
    "        self.encoder = Encoder(channel_in, ch=ch, blocks=blocks, num_features=num_features)\n",
    "        self.decoder = Decoder(channel_in, ch=ch, blocks=blocks, num_features=num_features)\n",
    "\n",
    "    def forward(self, x, index):\n",
    "        index_features = self.time_mlp(index)\n",
    "        \n",
    "        bottleneck, skip_list = self.encoder(x, index_features)\n",
    "        recon_img = self.decoder(bottleneck, skip_list, index_features)\n",
    "        return recon_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_alphas_bar(timesteps, s=0.008):\n",
    "    steps = timesteps + 1\n",
    "    x = torch.linspace(0, steps, steps)\n",
    "    alphas_bar = torch.cos(((x / steps) + s) / (1 + s) * torch.pi * 0.5) ** 2\n",
    "    alphas_bar = alphas_bar / alphas_bar[0]\n",
    "    return alphas_bar[:timesteps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise_from_x0(curr_img, img_pred, alpha):\n",
    "    return (curr_img - alpha.sqrt() * img_pred)/((1 - alpha).sqrt() + 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cold_diffuse(diffusion_model, sample_in, total_steps):\n",
    "    diffusion_model.eval()\n",
    "    bs = sample_in.shape[0]\n",
    "    alphas = torch.flip(cosine_alphas_bar(total_steps), (0,)).to(device)\n",
    "    random_sample = copy.deepcopy(sample_in)\n",
    "    with torch.no_grad():\n",
    "        for i in trange(total_steps - 1):\n",
    "            index = (i * torch.ones(bs, device=sample_in.device)).long()\n",
    "\n",
    "            img_output = diffusion_model(random_sample, index)\n",
    "\n",
    "            noise = noise_from_x0(random_sample, img_output, alphas[i])\n",
    "            x0 = img_output\n",
    "\n",
    "            rep1 = alphas[i].sqrt() * x0 + (1 - alphas[i]).sqrt() * noise\n",
    "            rep2 = alphas[i + 1].sqrt() * x0 + (1 - alphas[i + 1]).sqrt() * noise\n",
    "\n",
    "            random_sample += rep2 - rep1\n",
    "\n",
    "        index = ((total_steps - 1) * torch.ones(bs, device=sample_in.device)).long()\n",
    "        img_output = diffusion_model(random_sample, index)\n",
    "\n",
    "    return img_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataloader itterable object\n",
    "dataiter = iter(train_loader)\n",
    "# Sample from the itterable object\n",
    "images, labels = next(dataiter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps = 50\n",
    "\n",
    "# network\n",
    "u_net = Unet(channel_in=images.shape[1], ch=32, blocks=(1, 2, 4), timesteps=timesteps).to(device)\n",
    "\n",
    "#A fixed latent noise vector so we can see the improvement over the epochs\n",
    "fixed_latent_noise = torch.randn(images.shape[0], images.shape[1], img_size, img_size).to(device)\n",
    "\n",
    "# Adam optimizer\n",
    "optimizer = optim.Adam(u_net.parameters(), lr=lr)\n",
    "lr_schedule = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=train_epoch, eta_min=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_log = []\n",
    "mean_loss = 0\n",
    "\n",
    "alphas = torch.flip(cosine_alphas_bar(timesteps), (0,)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.plot(alphas.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_batch = alphas.reshape(-1, 1, 1, 1)\n",
    "images_1 = images[0:1].expand(alpha_batch.shape[0], images.shape[1], img_size, img_size).to(device)\n",
    "latent_noise = torch.randn_like(images_1)\n",
    "noise_input = alpha_batch.sqrt() * images_1 + (1 - alpha_batch).sqrt() * latent_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets visualise an entire batch of images!\n",
    "plt.figure(figsize = (20,10))\n",
    "out = torchvision.utils.make_grid(torch.clamp(noise_input, -1, 1).cpu(), 8, normalize=True)\n",
    "_ = plt.imshow(out.numpy().transpose((1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = images.shape[0]\n",
    "images = images.to(device)\n",
    "\n",
    "rand_index = torch.randint(timesteps, (bs, ), device=device)\n",
    "alpha_batch = alphas[rand_index].reshape(bs, 1, 1, 1)\n",
    "noise_input = alpha_batch.sqrt() * images + (1 - alpha_batch).sqrt() * fixed_latent_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets visualise an entire batch of images!\n",
    "plt.figure(figsize = (20,10))\n",
    "out = torchvision.utils.make_grid(torch.clamp(noise_input, -1, 1)[:16].cpu(), 8, normalize=True)\n",
    "_ = plt.imshow(out.numpy().transpose((1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbar = trange(train_epoch, leave=False, desc=\"Epoch\")    \n",
    "u_net.train()\n",
    "for epoch in pbar:\n",
    "    pbar.set_postfix_str('Loss: %.4f' % (mean_loss/len(train_loader)))\n",
    "    mean_loss = 0\n",
    "\n",
    "    for num_iter, (images, labels) in enumerate(tqdm(train_loader, leave=False)):\n",
    "\n",
    "        images = images.to(device)\n",
    "        \n",
    "        #the size of the current minibatch\n",
    "        bs = images.shape[0]\n",
    "\n",
    "        rand_index = torch.randint(timesteps, (bs, ), device=device)\n",
    "        random_sample = torch.randn_like(images)\n",
    "        alpha_batch = alphas[rand_index].reshape(bs, 1, 1, 1)\n",
    "        \n",
    "        noise_input = alpha_batch.sqrt() * images + (1 - alpha_batch).sqrt() * random_sample\n",
    "\n",
    "        img_pred = u_net(noise_input, rand_index)\n",
    "        \n",
    "        loss = F.l1_loss(img_pred, images)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        #log the generator training loss\n",
    "        loss_log.append(loss.item())\n",
    "        mean_loss += loss.item()\n",
    "        \n",
    "    lr_schedule.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,10))\n",
    "out = vutils.make_grid(torch.clamp(noise_input, -1, 1).detach().cpu(), nrow=16, normalize=True)\n",
    "_ = plt.imshow(out.numpy().transpose((1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,10))\n",
    "out = vutils.make_grid(img_pred.detach().cpu(), nrow=16, normalize=True)\n",
    "_ = plt.imshow(out.numpy().transpose((1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_sample = cold_diffuse(u_net, fixed_latent_noise, total_steps=timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,10))\n",
    "out = vutils.make_grid(fake_sample.detach().cpu(), nrow=16, normalize=True)\n",
    "_ = plt.imshow(out.numpy().transpose((1, 2, 0)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
