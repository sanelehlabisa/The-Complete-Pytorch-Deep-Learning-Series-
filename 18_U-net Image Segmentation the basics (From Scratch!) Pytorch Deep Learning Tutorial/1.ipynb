{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Segmentation in Pytorch using UNet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'Trainer'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 25\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcopy\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mTrainer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ModelTrainer\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mDatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CUB200\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'Trainer'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader as dataloader\n",
    "import torchvision.models as models\n",
    "\n",
    "# You'll need to install albumentations!\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import trange, tqdm\n",
    "from PIL import Image, ImageOps\n",
    "import copy\n",
    "import pandas as pd\n",
    "\n",
    "from Trainer import ModelTrainer\n",
    "from Datasets import CUB200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The size of our mini batches\n",
    "batch_size = 4\n",
    "\n",
    "# How many itterations of our dataset\n",
    "num_epochs = 32\n",
    "\n",
    "# Optimizer learning rate\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# You'll need to Download Dataset found here\n",
    "# https://www.kaggle.com/datasets/wenewone/cub2002011\n",
    "# Unzip and rename to cub_200\n",
    "# Where to load/save the dataset from \n",
    "data_set_root = \"../datasets/cub_200\"\n",
    "\n",
    "# What to resize our images to \n",
    "image_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_from_checkpoint = False\n",
    "\n",
    "save_dir = '../data/Models'\n",
    "model_name = 'UNet_CUB'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device to GPU_indx if GPU is avaliable\n",
    "gpu_indx = 0\n",
    "device = torch.device(gpu_indx if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only include the augmentations if you can use the v2 transforms that will augment \n",
    "# both the image and bounding boxes (you'll need to modify the dataset class too!)\n",
    "\n",
    "train_transform = A.Compose([A.SmallestMaxSize(max_size=image_size),\n",
    "                             A.RandomCrop(height=image_size, width=image_size),\n",
    "                             A.HorizontalFlip(p=0.5),\n",
    "                             A.ShiftScaleRotate(shift_limit=0.2, scale_limit=0.2, rotate_limit=30, p=0.5),\n",
    "                             A.RGBShift(r_shift_limit=25, g_shift_limit=25, b_shift_limit=25, p=0.5),\n",
    "                             A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, p=0.5),\n",
    "                             A.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                         std=[0.229, 0.224, 0.225]),\n",
    "                            ToTensorV2()], \n",
    "                            bbox_params=A.BboxParams(format='coco',\n",
    "                                                     min_area=0, min_visibility=0.0, \n",
    "                                                     label_fields=['class_labels']))\n",
    "\n",
    "transform = A.Compose([A.SmallestMaxSize(max_size=image_size),\n",
    "                       A.CenterCrop(height=image_size, width=image_size),\n",
    "                       A.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                   std=[0.229, 0.224, 0.225]),\n",
    "                       ToTensorV2()], \n",
    "                      bbox_params=A.BboxParams(format='coco',\n",
    "                                               min_area=0, min_visibility=0.0, \n",
    "                                               label_fields=['class_labels']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.Module class that will return the IoU for a batch of outputs\n",
    "class MaskIOU(nn.Module):\n",
    "\n",
    "    def mask_intersection_over_union(self, pred_bbox, target_bbox):\n",
    "\n",
    "        # compute the area of intersection rectangle\n",
    "        interArea = (pred_bbox * target_bbox).sum(dim=[1, 2])\n",
    "\n",
    "        area1 = pred_bbox.sum(dim=[1, 2])\n",
    "        area2 = target_bbox.sum(dim=[1, 2])\n",
    "\n",
    "        # compute the intersection over union by taking the intersection\n",
    "        # area and dividing it by the sum of prediction + ground-truth\n",
    "        # areas - the interesection area\n",
    "        iou = interArea / (area1 + area2 - interArea + 1e-5)\n",
    "\n",
    "        # return the intersection over union value\n",
    "        return iou\n",
    "\n",
    "    def forward(self, predictions, data):\n",
    "        \"\"\"\n",
    "        data: list of data, index 0 is the input image index [0] is the target\n",
    "        predictions: raw output of the model\n",
    "        \"\"\"\n",
    "        \n",
    "        pred_mask = predictions.argmax(1)\n",
    "        target_mask = data[1].to(pred_mask.device)\n",
    "        \n",
    "        return self.mask_intersection_over_union(pred_mask, target_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our Datasets\n",
    "# You'll need to download the dataset from Kaggle\n",
    "# https://www.kaggle.com/datasets/wenewone/cub2002011\n",
    "# Unzip it (and the directories it contains) into the datasets directory \n",
    "# and rename the top-level directory cub_200\n",
    "\n",
    "train_data = CUB200(data_set_root, image_size=image_size, transform=train_transform, \n",
    "                    test_train=0, return_masks=True)\n",
    "\n",
    "test_data = CUB200(data_set_root, image_size=image_size, transform=transform, \n",
    "                   test_train=1, return_masks=True)\n",
    "\n",
    "# Split trainging data into train and validation set with 90/10% traning/validation split\n",
    "validation_split = 0.9\n",
    "\n",
    "n_train_examples = int(len(train_data)*validation_split)\n",
    "n_valid_examples = len(train_data) - n_train_examples\n",
    "train_data, valid_data = torch.utils.data.random_split(train_data, [n_train_examples, n_valid_examples],\n",
    "                                                       generator=torch.Generator().manual_seed(42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Unet\n",
    "class UnetDown(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(UnetDown, self).__init__()\n",
    "        \n",
    "        model = [nn.BatchNorm2d(input_size),\n",
    "                 nn.ELU(),\n",
    "                 nn.Conv2d(input_size, output_size, kernel_size=3, stride=1, padding=1),\n",
    "                 nn.BatchNorm2d(output_size),\n",
    "                 nn.ELU(),\n",
    "                 nn.MaxPool2d(2),\n",
    "                 nn.Conv2d(output_size, output_size, kernel_size=3, stride=1, padding=1)]\n",
    "        \n",
    "        self.model = nn.Sequential(*model)\n",
    "        \n",
    "    def forward(self, x):        \n",
    "        return self.model(x)\n",
    "      \n",
    "\n",
    "class UnetUp(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(UnetUp, self).__init__()\n",
    "\n",
    "        model = [nn.BatchNorm2d(input_size),\n",
    "                 nn.ELU(),\n",
    "                 nn.Conv2d(input_size, output_size, kernel_size=3, stride=1, padding=1),\n",
    "                 nn.BatchNorm2d(output_size),\n",
    "                 nn.ELU(),\n",
    "                 nn.Upsample(scale_factor=2, mode=\"nearest\"),\n",
    "                 nn.Conv2d(output_size, output_size, kernel_size=3, stride=1, padding=1)]\n",
    "          \n",
    "        self.model = nn.Sequential(*model)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "            \n",
    "         \n",
    "class Unet(nn.Module):\n",
    "    def __init__(self, channels_in, channels_out=2):\n",
    "        super(Unet, self).__init__()\n",
    "        \n",
    "        self.conv_in = nn.Conv2d(channels_in, 64, \n",
    "                                 kernel_size=3, stride=1, padding=1)   # H X W --> H X W\n",
    "        \n",
    "        self.down1 = UnetDown(64, 64)  #  H   X W   --> H/2 X W/2\n",
    "        self.down2 = UnetDown(64, 128)  #  H/2 X W/2 --> H/4 X W/4\n",
    "        self.down3 = UnetDown(128, 128)  #  H/4 X W/4 --> H/8 X W/8\n",
    "        self.down4 = UnetDown(128, 256)  # H/8 X W/8 --> H/16 X W/16\n",
    "\n",
    "        self.up4 = UnetUp(256, 128)  #    H/16 X W/16 --> H/8 X W/8\n",
    "        self.up5 = UnetUp(128 * 2, 128)  # H/8 X W/8 --> H/4 X W/4\n",
    "        self.up6 = UnetUp(128 * 2, 64)  # H/4 X W/4 --> H/2 X W/2\n",
    "        self.up7 = UnetUp(64 * 2, 64)  # H/2 X W/2 --> H   X W\n",
    "        \n",
    "        self.conv_out = nn.Conv2d(64 * 2, channels_out, \n",
    "                                  kernel_size=3, stride=1, padding=1)  # H X W --> H X W\n",
    "\n",
    "    def forward(self, x):\n",
    "        x0 = self.conv_in(x)  # 16 x H x W\n",
    "        \n",
    "        x1 = self.down1(x0)  # 32 x H/2 x W/2\n",
    "        x2 = self.down2(x1)  # 64 x H/4 x W/4\n",
    "        x3 = self.down3(x2)  # 64 x H/8 x W/8\n",
    "        x4 = self.down4(x3)  # 128 x H/16 x W/16\n",
    "\n",
    "        # Bottle-neck --> 128 x H/16 x W/16\n",
    "\n",
    "        x5 = self.up4(x4)  # 64 x H/8 x W/8\n",
    "        \n",
    "        x5_ = torch.cat((x5, x3), 1)  # 128 x H/8 x W/8\n",
    "        x6 = self.up5(x5_)  # 32 x H/4 x W/4\n",
    "        \n",
    "        x6_ = torch.cat((x6, x2), 1)  # 64 x H/4 x W/4\n",
    "        x7 = self.up6(x6_)  # 16 x H/2 x W/2\n",
    "        \n",
    "        x7_ = torch.cat((x7, x1), 1)  # 64 x H/2 x W/2\n",
    "        x8 = self.up7(x7_)  # 16 x H x W\n",
    "        \n",
    "        x8_ = F.elu(torch.cat((x8, x0), 1))  # 32 x H x W        \n",
    "        return self.conv_out(x8_)  # Co x H x W\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet = Unet(channels_in=3, channels_out=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_trainer = ModelTrainer(model=unet.to(device), output_size=-1, device=device, \n",
    "                             loss_fun=nn.CrossEntropyLoss(), batch_size=batch_size, \n",
    "                             learning_rate=learning_rate, save_dir=save_dir, model_name=model_name,\n",
    "                             eval_metric=MaskIOU(), start_from_checkpoint=start_from_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_trainer.set_data(train_set=train_data, test_set=test_data, val_set=valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_trainer.set_lr_schedule(optim.lr_scheduler.StepLR(model_trainer.optimizer, \n",
    "                                                        step_size=1, \n",
    "                                                        gamma=0.95))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,10))\n",
    "images, mask, bbox, labels = next(iter(model_trainer.train_loader))\n",
    "out = torchvision.utils.make_grid(images[0:16], normalize=True)\n",
    "_ = plt.imshow(out.numpy().transpose((1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,10))\n",
    "out = torchvision.utils.make_grid((mask[0:16]).unsqueeze(1).float(), normalize=True)\n",
    "_ = plt.imshow(out.numpy().transpose((1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets see how many Parameter's our Model has!\n",
    "num_params = 0\n",
    "for param in model_trainer.model.parameters():\n",
    "    num_params += param.flatten().shape[0]\n",
    "print(\"This model has %d (approximately %d Million) Parameters!\" % (num_params, num_params//1e6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_trainer.run_training(num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The highest validation IoU was %.2f\" %(model_trainer.best_valid_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.figure(figsize = (10,5))\n",
    "_ = plt.plot(model_trainer.train_loss_logger)\n",
    "_ = plt.title(\"Training Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.figure(figsize = (10,5))\n",
    "_ = plt.plot(model_trainer.train_acc_logger, c = \"y\")\n",
    "_ = plt.plot(model_trainer.val_acc_logger, c = \"k\")\n",
    "\n",
    "_ = plt.title(\"Average IoU\")\n",
    "_ = plt.legend([\"Training IoU\", \"Validation IoU\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, mask, bbox, labels = next(iter(model_trainer.test_loader))\n",
    "model_trainer.eval()\n",
    "with torch.no_grad():\n",
    "    pred_out = model_trainer(images.to(device)).argmax(1).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,10))\n",
    "out = torchvision.utils.make_grid(images[0:16], normalize=True)\n",
    "_ = plt.imshow(out.numpy().transpose((1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,10))\n",
    "out = torchvision.utils.make_grid((mask[0:16]).unsqueeze(1).float(), normalize=True)\n",
    "_ = plt.imshow(out.numpy().transpose((1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,10))\n",
    "out = torchvision.utils.make_grid(pred_out[0:16].unsqueeze(1).float(), normalize=True)\n",
    "_ = plt.imshow(out.numpy().transpose((1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the evaluate function and pass the evaluation/test dataloader etc\n",
    "test_acc = model_trainer.evaluate_model(train_test_val=\"test\")\n",
    "print(\"The Test Average IoU is: %.2f\" %(test_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
