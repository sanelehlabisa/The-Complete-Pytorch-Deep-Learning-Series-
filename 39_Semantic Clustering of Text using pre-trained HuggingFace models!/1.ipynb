{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tqdm.notebook import trange, tqdm\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torchtext.datasets import AG_NEWS, IMDB\n",
    "\n",
    "# Make sure you are using the lastest version!\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approx number of text samples to use\n",
    "num_data_points = 10000\n",
    "\n",
    "# Define the batch size for mini-batch gradient descent\n",
    "batch_size = 64\n",
    "\n",
    "# https://www.kaggle.com/datasets/ltcmdrdata/plain-text-wikipedia-202011\n",
    "# Define the root directory of the dataset\n",
    "data_set_root = \"../datasets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available, set device accordingly\n",
    "device = torch.device(1 if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subtract 1 from all labels to make them 0 and 1 (not 1 and 2...)\n",
    "# make everything lowercase\n",
    "def process_data(x):\n",
    "    return x[0] - 1, x[1].lower(), \n",
    "\n",
    "dataset_train = IMDB(root=data_set_root, split=\"train\")\n",
    "dataset_test = IMDB(root=data_set_root, split=\"test\")\n",
    "    \n",
    "dataset_train = dataset_train.map(process_data)\n",
    "dataset_test = dataset_test.map(process_data)\n",
    "\n",
    "# IMDB does not seem to be properly shuffled....\n",
    "dataset_train = dataset_train.shuffle(buffer_size=10000).set_shuffle(True)\n",
    "dataset_test = dataset_test.shuffle(buffer_size=10000).set_shuffle(True)\n",
    "\n",
    "# This is a hack to get around some random bug with the IMDB dataset not properly\n",
    "# Processing the positive (pos) datapoints, you only need to do this once...\n",
    "# This will take a few seconds..\n",
    "for label, text in dataset_train:\n",
    "    continue\n",
    "    \n",
    "for label, text in dataset_test:\n",
    "    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders for the training and testing datasets\n",
    "data_loader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "data_loader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=True, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model from HuggingFace Hub\n",
    "tokenizer = AutoTokenizer.from_pretrained('BAAI/bge-small-en-v1.5')\n",
    "model = AutoModel.from_pretrained('BAAI/bge-small-en-v1.5').to(device)\n",
    "_ = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_log = []\n",
    "labels_log = []\n",
    "text_log = []\n",
    "\n",
    "# Loop over each batch in the training dataset\n",
    "for label, text in tqdm(data_loader_train, desc=\"Extracting\", leave=False, total=num_data_points//batch_size):\n",
    "    encoded_input = tokenizer(text, padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "\n",
    "    with torch.cuda.amp.autocast():\n",
    "        with torch.no_grad():\n",
    "            embeddings = model(**encoded_input)[0][:, 0]\n",
    "            \n",
    "            norm_embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)\n",
    "            \n",
    "            embeddings_log.append(norm_embeddings.cpu())\n",
    "            labels_log.append(label)\n",
    "            text_log += list(text)\n",
    "            \n",
    "    if len(labels_log) * batch_size >= num_data_points:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_embeddings = torch.cat(embeddings_log).numpy()\n",
    "np_labels = torch.cat(labels_log).numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 2  # You can adjust this number based on your data\n",
    "kmeans = KMeans(n_clusters=n_clusters)\n",
    "cluster_labels = kmeans.fit_predict(np_embeddings)\n",
    "cluster_centers = kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA Dim reduction\n",
    "pca = PCA(n_components=20)\n",
    "\n",
    "# Stack embs and centers to project together\n",
    "combined_embs = np.vstack([np_embeddings, cluster_centers])\n",
    "combined_pca = pca.fit_transform(combined_embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TSNE Dim reduction (Maintain local distances)\n",
    "tsne = TSNE(n_components=2, perplexity=50)\n",
    "combined_2d = tsne.fit_transform(combined_pca)\n",
    "\n",
    "# Separate the projected embeddings and centers\n",
    "embeddings_2d = combined_2d[:-n_clusters]\n",
    "centers_2d = combined_2d[-n_clusters:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scatter plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "cluster_0_index = np.where(cluster_labels == 0)[0]\n",
    "scatter1 = plt.scatter(embeddings_2d[cluster_0_index, 0], embeddings_2d[cluster_0_index, 1], \n",
    "                      c=np_labels[cluster_0_index], s=5, marker=\"_\")\n",
    "\n",
    "cluster_1_index = np.where(cluster_labels == 1)[0]\n",
    "scatter2 = plt.scatter(embeddings_2d[cluster_1_index, 0], embeddings_2d[cluster_1_index, 1], \n",
    "                      c=np_labels[cluster_1_index], s=5, marker=\"o\")\n",
    "\n",
    "_ = plt.scatter(centers_2d[:, 0], centers_2d[:, 1], c=\"r\", s=100, marker=\"x\")\n",
    "_ = plt.legend([\"cluster 0\", \"cluster 1\"])\n",
    "_ = plt.xlabel('t-SNE feature 1')\n",
    "_ = plt.ylabel('t-SNE feature 2')\n",
    "_ = plt.title('t-SNE visualization of embeddings with Semantic label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the indices of all points in cluster 0\n",
    "cluster_0_indices = np.where(cluster_labels == 0)[0]\n",
    "\n",
    "# Get the labels and embeddings of points in cluster 0\n",
    "cluster_0_labels = np_labels[cluster_0_indices]\n",
    "cluster_0_points = np_embeddings[cluster_0_indices]\n",
    "\n",
    "# Find the most common semantic label for this cluster\n",
    "cluster_0_median_label = np.median(cluster_0_labels)\n",
    "print(\"The most common semantic label is %d\" % cluster_0_median_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.hist(cluster_0_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the cluster indices of points within this cluster that do not have the typical semantic label\n",
    "outlier_indices = np.where(~(cluster_0_labels == cluster_0_median_label))[0]\n",
    "\n",
    "# Get the origional indices for these points (to index text list)\n",
    "cluster_0_outlier_indices = cluster_0_indices[outlier_indices]\n",
    "\n",
    "# Get the embeddings of the outliers\n",
    "cluster_0_outlier_points = cluster_0_points[outlier_indices]\n",
    "cluster_0_outlier_labels = cluster_0_labels[outlier_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Find the distance between each outlier embedding and the cluster center for cluster 0\n",
    "points_diff = (cluster_0_outlier_points - cluster_centers[0].reshape(1, -1))\n",
    "points_dist = np.mean(np.power(points_diff, 2), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the outlier that is closest to the cluster center\n",
    "# AKA the \"worst\" outlier\n",
    "closest_5 = np.argsort(points_dist)[:5]\n",
    "closest_5_indices = cluster_0_outlier_indices[closest_5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "closest_5_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the origional text for this outlier\n",
    "outlier_text = text_log[closest_5_indices[0]]\n",
    "outlier_label = np_labels[closest_5_indices[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_text\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
