{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torch.utils.data.dataloader as dataloader\n",
    "\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from tqdm.notebook import trange, tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The size of our mini batches\n",
    "batch_size = 128\n",
    "\n",
    "# How many itterations of our dataset\n",
    "num_epochs = 50\n",
    "\n",
    "# Optimizer learning rate\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# Where to load/save the dataset from \n",
    "data_set_root = \"../../datasets\"\n",
    "\n",
    "# The size of each patch in the sequence\n",
    "# Our images are quite small so we'll use a smaller image patch size\n",
    "patch_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device to GPU_indx if GPU is avaliable\n",
    "gpu_indx = 0\n",
    "device = torch.device(gpu_indx if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare a composition of transforms\n",
    "# transforms.Compose will perform the transforms in order\n",
    "# NOTE: some transform only take in a PIL image, others only a Tensor\n",
    "# EG Resize and ToTensor take in a PIL Image, Normalize takes in a Tensor\n",
    "# Refer to documentation\n",
    "transform = transforms.Compose([transforms.AutoAugment(transforms.AutoAugmentPolicy.CIFAR10),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                     std=[0.229, 0.224, 0.225])]) \n",
    "\n",
    "test_transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                     transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                          std=[0.229, 0.224, 0.225])]) \n",
    "# Note: ToTensor() will scale unit8 and similar type data to a float and re-scale to 0-1\n",
    "# Note: We are normalizing with the dataset mean and std "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = datasets.CIFAR10(data_set_root, train=True, download=True, transform=transform)\n",
    "test_data = datasets.CIFAR10(data_set_root, train=False, download=True, transform=test_transform)\n",
    "\n",
    "# We are going to split the test dataset into a train and validation set 90%/10%\n",
    "validation_split = 0.9\n",
    "\n",
    "# Determine the number of samples for each split\n",
    "n_train_examples = int(len(train_data) * validation_split)\n",
    "n_valid_examples = len(train_data) - n_train_examples\n",
    "\n",
    "# The function random_split will take our dataset and split it randomly and give us dataset\n",
    "# that are the sizes we gave it\n",
    "# Note: we can split it into to more then two pieces!\n",
    "train_data, valid_data = torch.utils.data.random_split(train_data, [n_train_examples, n_valid_examples],\n",
    "                                                       generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "# IMPORTANT TO KNOW!!!!!!!!!\n",
    "# Here we pass the random_split function a manual seed, this is very important as if we did not do this then \n",
    "# everytime we randomly split our training and validation set we would get different splits!!!\n",
    "# For example if we saved our model and reloaded it in the future to train some more, the dataset that we now use to\n",
    "# train with will undoubtably contain datapoints that WERE in the validation set initially!!\n",
    "# Our model would therefore be trained with both validation and training data -- very bad!!!\n",
    "# Setting the manual seed to the same value everytime prevents this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of training examples: {len(train_data)}')\n",
    "print(f'Number of validation examples: {len(valid_data)}')\n",
    "print(f'Number of testing examples: {len(test_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the training, Validation and Evaluation/Test Datasets\n",
    "# It is best practice to separate your data into these three Datasets\n",
    "# Though depending on your task you may only need Training + Evaluation/Test or maybe only a Training set\n",
    "# (It also depends on how much data you have)\n",
    "# https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataloader\n",
    "train_loader = dataloader.DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "valid_loader = dataloader.DataLoader(valid_data, batch_size=batch_size)\n",
    "test_loader  = dataloader.DataLoader(test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_patches(image_tensor, patch_size=8):\n",
    "    # Get the dimensions of the image tensor\n",
    "    bs, c, h, w = image_tensor.size()\n",
    "    \n",
    "    # Define the Unfold layer with appropriate parameters\n",
    "    unfold = torch.nn.Unfold(kernel_size=patch_size, stride=patch_size)\n",
    "    \n",
    "    # Apply Unfold to the image tensor\n",
    "    unfolded = unfold(image_tensor)\n",
    "    \n",
    "    # Reshape the unfolded tensor to match the desired output shape\n",
    "    # Output shape: BSxLxH, where L is the number of patches in each dimension\n",
    "    unfolded = unfolded.transpose(1, 2).reshape(bs, -1, c * patch_size * patch_size)\n",
    "    \n",
    "    return unfolded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataloader itterable object\n",
    "dataiter = next(iter(test_loader))\n",
    "# Sample from the itterable object\n",
    "test_images, test_labels = dataiter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract patches from the test images using the defined function\n",
    "patches = extract_patches(test_images, patch_size=patch_size)\n",
    "\n",
    "patches_square = patches.reshape(test_images.shape[0], -1, 3, patch_size, patch_size)\n",
    "\n",
    "# Calculate the grid size for visualization\n",
    "grid_size = test_images.shape[2] // patch_size\n",
    "print(\"Sequence Length %d\" % (grid_size**2))\n",
    "\n",
    "# Visualize the patches as a grid\n",
    "plt.figure(figsize=(5, 5))\n",
    "out = torchvision.utils.make_grid(patches_square[0], grid_size, normalize=True, pad_value=0.5)\n",
    "_ = plt.imshow(out.numpy().transpose((1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer block with self-attention\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, hidden_size=128, num_heads=4):\n",
    "        # Initialize the parent nn.Module\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        \n",
    "        # Layer normalization to normalize the input data\n",
    "        self.norm1 = nn.LayerNorm(hidden_size)\n",
    "        \n",
    "        # Multi-head attention mechanism\n",
    "        self.multihead_attn = nn.MultiheadAttention(hidden_size, num_heads=num_heads, \n",
    "                                                    batch_first=True, dropout=0.1)\n",
    "        \n",
    "        # Another layer normalization\n",
    "        self.norm2 = nn.LayerNorm(hidden_size)\n",
    "        \n",
    "        # Multi-layer perceptron (MLP) with a hidden layer and activation function\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size * 2),\n",
    "            nn.LayerNorm(hidden_size * 2),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(hidden_size * 2, hidden_size)\n",
    "        )\n",
    "                \n",
    "    def forward(self, x):\n",
    "        # Apply the first layer normalization\n",
    "        norm_x = self.norm1(x)\n",
    "        \n",
    "        # Apply multi-head attention and add the input (residual connection)\n",
    "        x = self.multihead_attn(norm_x, norm_x, norm_x)[0] + x\n",
    "        \n",
    "        # Apply the second layer normalization\n",
    "        norm_x = self.norm2(x)\n",
    "        \n",
    "        # Pass through the MLP and add the input (residual connection)\n",
    "        x = self.mlp(norm_x) + x\n",
    "        \n",
    "        return x\n",
    "    \n",
    "\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, image_size, channels_in, patch_size, hidden_size, num_layers, num_heads=8):\n",
    "        # Call the __init__ function of the parent nn.Module class\n",
    "        super(ViT, self).__init__()\n",
    "        \n",
    "        self.patch_size = patch_size\n",
    "        \n",
    "        # Fully connected layer to project input patches to the hidden size dimension\n",
    "        self.fc_in = nn.Linear(channels_in * patch_size * patch_size, hidden_size)\n",
    "        \n",
    "        # Create a list of Transformer blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(hidden_size, num_heads) for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Fully connected output layer to map to the number of classes (e.g., 10 for CIFAR-10)\n",
    "        self.fc_out = nn.Linear(hidden_size, 10)\n",
    "        \n",
    "        # Parameter for the output token\n",
    "        self.out_vec = nn.Parameter(torch.zeros(1, 1, hidden_size))\n",
    "        \n",
    "        # Positional embeddings to retain positional information of patches\n",
    "        seq_length = (image_size // patch_size) ** 2\n",
    "        self.pos_embedding = nn.Parameter(torch.empty(1, seq_length, hidden_size).normal_(std=0.001))\n",
    "\n",
    "    def forward(self, image):\n",
    "        bs = image.shape[0]\n",
    "\n",
    "        # Extract patches from the image and flatten them\n",
    "        patch_seq = extract_patches(image, patch_size=self.patch_size)\n",
    "        \n",
    "        # Project patches to the hidden size dimension\n",
    "        patch_emb = self.fc_in(patch_seq)\n",
    "\n",
    "        # Add positional embeddings to the patch embeddings\n",
    "        patch_emb = patch_emb + self.pos_embedding\n",
    "        \n",
    "        # Concatenate the output token to the patch embeddings\n",
    "        embs = torch.cat((self.out_vec.expand(bs, 1, -1), patch_emb), 1)\n",
    "\n",
    "        # Pass the embeddings through each Transformer block\n",
    "        for block in self.blocks:\n",
    "            embs = block(embs)\n",
    "        \n",
    "        # Use the embedding of the output token for classification\n",
    "        return self.fc_out(embs[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataloader itterable object\n",
    "dataiter = next(iter(train_loader))\n",
    "# Sample from the itterable object\n",
    "train_images, train_labels = dataiter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets visualise an entire batch of images!\n",
    "plt.figure(figsize = (20,10))\n",
    "out = torchvision.utils.make_grid(train_images, 8, normalize=True)\n",
    "_ = plt.imshow(out.numpy().transpose((1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of our network\n",
    "# Set channels_in to the number of channels of the dataset images (1 channel for MNIST)\n",
    "model = ViT(image_size=test_images.shape[2], \n",
    "            channels_in=test_images.shape[1], \n",
    "            patch_size=patch_size, \n",
    "            hidden_size=128,\n",
    "            num_layers=8,\n",
    "            num_heads=8).to(device)\n",
    "\n",
    "# View the network\n",
    "# Note that the layer order is simply the order in which we defined them, NOT the order of the forward pass\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how many Parameters our Model has!\n",
    "num_model_params = 0\n",
    "for param in model.parameters():\n",
    "    num_model_params += param.flatten().shape[0]\n",
    "\n",
    "print(\"-This Model Has %d (Approximately %d Million) Parameters!\" % (num_model_params, num_model_params//1e6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass image through network\n",
    "out = model(test_images.to(device))\n",
    "# Check output\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass our network parameters to the optimiser set our lr as the learning_rate\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "lr_scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, \n",
    "                                                    T_max=num_epochs, \n",
    "                                                    eta_min=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Cross Entropy Loss\n",
    "loss_fun = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function should perform a single training epoch using our training data\n",
    "def train(model, optimizer, loader, device, loss_fun, loss_logger):\n",
    "    \n",
    "    # Set Network in train mode\n",
    "    model.train()\n",
    "    for i, (x, y) in enumerate(tqdm(loader, leave=False, desc=\"Training\")):\n",
    "        # Forward pass of image through network and get output\n",
    "        fx = model(x.to(device))\n",
    "        \n",
    "        # Calculate loss using loss function\n",
    "        loss = loss_fun(fx, y.to(device))\n",
    "\n",
    "        # Zero Gradents\n",
    "        optimizer.zero_grad()\n",
    "        # Backpropagate Gradents\n",
    "        loss.backward()\n",
    "        # Do a single optimization step\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Log the loss for plotting\n",
    "        loss_logger.append(loss.item())\n",
    "        \n",
    "    # Return the avaerage loss and acc from the epoch as well as the logger array       \n",
    "    return model, optimizer, loss_logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function should perform a single evaluation epoch, it WILL NOT be used to train our model\n",
    "def evaluate(model, device, loader):\n",
    "    \n",
    "    # Initialise counter\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    # Set network in evaluation mode\n",
    "    # Layers like Dropout will be disabled\n",
    "    # Layers like Batchnorm will stop calculating running mean and standard deviation\n",
    "    # and use current stored values (More on these layer types soon!)\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (x, y) in enumerate(tqdm(loader, leave=False, desc=\"Evaluating\")):\n",
    "            # Forward pass of image through network\n",
    "            fx = model(x.to(device))\n",
    "            \n",
    "            # Log the cumulative sum of the acc\n",
    "            epoch_acc += (fx.argmax(1) == y.to(device)).sum().item()\n",
    "            \n",
    "    # Return the accuracy from the epoch     \n",
    "    return epoch_acc / len(loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loss_logger = []\n",
    "validation_acc_logger = []\n",
    "training_acc_logger = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_acc = 0\n",
    "train_acc = 0\n",
    "\n",
    "# This cell implements our training loop\n",
    "pbar = trange(0, num_epochs, leave=False, desc=\"Epoch\")    \n",
    "for epoch in pbar:\n",
    "    pbar.set_postfix_str('Accuracy: Train %.2f%%, Val %.2f%%' % (train_acc * 100, valid_acc * 100))\n",
    "\n",
    "    # Call the training function and pass training dataloader etc\n",
    "    model, optimizer, training_loss_logger = train(model=model, \n",
    "                                                   optimizer=optimizer, \n",
    "                                                   loader=train_loader, \n",
    "                                                   device=device, \n",
    "                                                   loss_fun=loss_fun, \n",
    "                                                   loss_logger=training_loss_logger)\n",
    "\n",
    "    # Call the evaluate function and pass the dataloader for both validation and training\n",
    "    train_acc = evaluate(model=model, device=device, loader=train_loader)\n",
    "    valid_acc = evaluate(model=model, device=device, loader=valid_loader)\n",
    "\n",
    "    # Log the train and validation accuracies\n",
    "    validation_acc_logger.append(valid_acc)\n",
    "    training_acc_logger.append(train_acc)\n",
    "    \n",
    "    # Reduce learning rate\n",
    "    lr_scheduler.step()\n",
    "\n",
    "print(\"Training Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,5))\n",
    "train_x = np.linspace(0, num_epochs, len(training_loss_logger))\n",
    "plt.plot(train_x, training_loss_logger)\n",
    "_ = plt.title(\"ViT Training Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,5))\n",
    "train_x = np.linspace(0, num_epochs, len(training_acc_logger))\n",
    "plt.plot(train_x, training_acc_logger, c = \"y\")\n",
    "valid_x = np.linspace(0, num_epochs, len(validation_acc_logger))\n",
    "plt.plot(valid_x, validation_acc_logger, c = \"k\")\n",
    "\n",
    "plt.title(\"ViT\")\n",
    "_ = plt.legend([\"Training accuracy\", \"Validation accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the evaluate function and pass the evaluation/test dataloader etc\n",
    "test_acc = evaluate(model=model, device=device, loader=test_loader)\n",
    "print(\"The total test accuracy is: %.2f%%\" %(test_acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets visualise the prediction for a few test images!\n",
    "with torch.no_grad():\n",
    "    fx = model(test_images[:8].to(device))\n",
    "    pred = fx.argmax(-1)\n",
    "    \n",
    "plt.figure(figsize = (20,10))\n",
    "out = torchvision.utils.make_grid(test_images[:8], 8, normalize=True)\n",
    "plt.imshow(out.numpy().transpose((1, 2, 0)))\n",
    "\n",
    "print(\"Predicted Values\\n\", list(pred.cpu().numpy()))\n",
    "print(\"True Values\\n\", list(test_labels[:8].numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the positional embeddings\n",
    "pos_embs = model.pos_embedding.detach().cpu()\n",
    "# Calculate the cosine similarity between every positional embedding\n",
    "dist = F.cosine_similarity(pos_embs, pos_embs.reshape(64, 1, 128), dim=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rows_cols = 32//patch_size\n",
    "fig, axes = plt.subplots(n_rows_cols, n_rows_cols, figsize=(5, 5))\n",
    "for i in range(n_rows_cols):\n",
    "    for j in range(n_rows_cols):\n",
    "        # Generate a sample image\n",
    "        img = dist[j + i * n_rows_cols].reshape(n_rows_cols, n_rows_cols)\n",
    "        \n",
    "        # Display the image\n",
    "        axes[i, j].imshow(img)\n",
    "        axes[i, j].axis('off')  # Hide the axes\n",
    "\n",
    "# Adjust layout and show the grid\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
