PyTorch Tutorial: Variational Autoencoders (VAEs)
Overview
The tutorial introduces Variational Autoencoders (VAEs), explaining how they differ from traditional autoencoders and how they can be implemented in PyTorch. The focus is on creating a denser latent representation that allows for effective sampling and generation of new data.
Key Concepts
Difference from Traditional Autoencoders

Traditional autoencoders map inputs to a single latent representation. In contrast, VAEs model each latent variable as a distribution (specifically, a Gaussian distribution parameterized by mean μ and log variance log(σ * σ)).
This allows for sampling from the latent space, enabling the generation of new data points.
Latent Space Representation

The VAE aims to create a continuous and dense latent space where every point corresponds to a valid output image.
Instead of having sparse representations, VAEs fill the latent space, allowing for easier sampling and generating valid outputs.
KL Divergence Penalty

To ensure that the learned latent distribution approximates a standard normal distribution (mean = 0, variance = 1), VAEs use a KL Divergence penalty.
The KL Divergence measures how one probability distribution diverges from a second expected probability distribution. The simplified formula used in VAEs is:

Dkl(q(z∣x)∣∣p(z))= 1 / 2 * (μ * μ + σ * σ − log(σ * σ) −1)

This penalty encourages the encoder to produce outputs that fit a standard normal distribution.
Loss Function

The overall loss function for the VAE combines the reconstruction loss (mean squared error) and the KL Divergence penalty:

def vae_loss(recon_x, x, mu, log_var):
    BCE = F.mse_loss(recon_x, x, reduction='sum')
    KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())
    return BCE + KLD
Network Architecture

The architecture consists of an encoder and a decoder:

Encoder: Outputs two vectors, one for μ and one for  log(σ * σ).
Decoder: Takes sampled latent variables and reconstructs the input.
Nearest neighbor upsampling is used instead of transpose convolutions in the decoder to maintain simplicity and avoid artifacts.

class VAE(nn.Module):
    def __init__(self):
        super(VAE, self).__init__()
        # Define encoder and decoder networks
        # Encoder outputs mu and log_var
        # Decoder reconstructs the input from sampled latent variables
    
    def encode(self, x):
        # Output mu and log_var
    
    def reparameterize(self, mu, log_var):
        std = torch.exp(0.5 * log_var)
        eps = torch.randn_like(std)
        return mu + eps * std  # Reparameterization trick

    def decode(self, z):
        # Reconstruct the input from latent representation
Sampling from Latent Space

During training, the model samples from the latent space using the reparameterization trick, which allows gradients to flow through the sampling process.
Training Process

The training loop is similar to that of traditional autoencoders but incorporates the VAE loss function.
The model is trained on the MNIST dataset, with the goal of reconstructing the input images from their latent representations.
Results and Observations

The tutorial showcases the model's ability to generate images by sampling from the learned latent space.
It highlights the challenge of generating high-quality images, noting that as latent distributions become denser, the decoder may struggle to distinguish between similar latent representations.
Interpolation in Latent Space

The tutorial also demonstrates how to interpolate between two latent representations, allowing for smooth transitions between different generated images. This is done by averaging the latent representations of different classes.

# Example of interpolation
interpolated = (latent_rep1 + latent_rep2) / 2
reconstructed_image = decoder(interpolated)
Future Directions

The tutorial touches on advanced topics such as hierarchical VAEs and diffusion models, which build upon the concepts introduced in this session.
Conclusion
The tutorial provides a comprehensive introduction to Variational Autoencoders, emphasizing their architecture, loss function, and the importance of a well-structured latent space for effective data generation.
For further exploration, the tutorial recommends additional resources for understanding the theoretical underpinnings of VAEs and their applications.