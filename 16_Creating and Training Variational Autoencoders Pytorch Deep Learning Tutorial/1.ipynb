{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variational Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.datasets as Datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import math\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm import trange, tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchSize = 64\n",
    "lr = 1e-4\n",
    "\n",
    "# Number of Training epochs\n",
    "nepoch = 10\n",
    "\n",
    "# The size of the Latent Vector\n",
    "latent_size = 128\n",
    "root = \"../datasets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "gpu_indx  = 0\n",
    "device = torch.device(gpu_indx if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define our transform\n",
    "# We'll upsample the images to 32x32 as it's easier to contruct our network\n",
    "transform = transforms.Compose([\n",
    "            transforms.Resize(32),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5], [0.5])])\n",
    "\n",
    "train_set = Datasets.MNIST(root=root, train=True, transform=transform, download=True)\n",
    "train_loader = DataLoader(train_set, batch_size=batchSize,shuffle=True, num_workers=4)\n",
    "\n",
    "test_set = Datasets.MNIST(root=root, train=False, transform=transform, download=True)\n",
    "test_loader = DataLoader(test_set, batch_size=batchSize, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss(recon, x, mu, logvar):\n",
    "    recon_loss = F.mse_loss(recon, x)\n",
    "    \n",
    "    # Here is our KL divergance loss implemented in code\n",
    "    # We will use the mean across the dimensions instead of the sum (which is common and would require different scaling)\n",
    "    kl_loss = -0.5 * (1 + logvar - mu.pow(2) - logvar.exp()).mean()\n",
    "    \n",
    "    # We'll tune the \"strength\" of KL divergance loss to get a good result \n",
    "    loss = recon_loss + 0.1 * kl_loss\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We split up our network into two parts, the Encoder and the Decoder\n",
    "class DownBlock(nn.Module):\n",
    "    def __init__(self, channels_in, channels_out):\n",
    "        super(DownBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(channels_in, channels_out, 3, 2, 1)\n",
    "        self.bn1 = nn.BatchNorm2d(channels_out)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(channels_out, channels_out, 3, 1, 1)\n",
    "        self.bn2 = nn.BatchNorm2d(channels_out)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(channels_in, channels_out, 3, 2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_skip = self.conv3(x)\n",
    "        \n",
    "        x = F.elu(self.bn1(self.conv1(x)))\n",
    "        x = self.conv2(x) + x_skip\n",
    "        \n",
    "        return F.elu(self.bn2(x))\n",
    "    \n",
    "    \n",
    "# We split up our network into two parts, the Encoder and the Decoder\n",
    "class UpBlock(nn.Module):\n",
    "    def __init__(self, channels_in, channels_out):\n",
    "        super(UpBlock, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(channels_in)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(channels_in, channels_in, 3, 1, 1)\n",
    "        self.bn2 = nn.BatchNorm2d(channels_in)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(channels_in, channels_out, 3, 1, 1)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(channels_in, channels_out, 3, 1, 1)\n",
    "        self.up_nn = nn.Upsample(scale_factor=2, mode=\"nearest\")\n",
    "\n",
    "    def forward(self, x_in):\n",
    "        x = F.elu(self.bn2(x_in))\n",
    "        \n",
    "        x_skip = self.up_nn(self.conv3(x))\n",
    "        \n",
    "        x = self.up_nn(F.elu(self.bn2(self.conv1(x))))\n",
    "        return self.conv2(x) + x_skip\n",
    "\n",
    "    \n",
    "# We split up our network into two parts, the Encoder and the Decoder\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, channels, ch=32, z=32):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv_1 = nn.Conv2d(channels, ch, 3, 1, 1)\n",
    "\n",
    "        self.conv_block1 = DownBlock(ch, ch)\n",
    "        self.conv_block2 = DownBlock(ch, ch * 2)\n",
    "        self.conv_block3 = DownBlock(ch * 2, ch * 4)\n",
    "\n",
    "        # Instead of flattening (and then having to unflatten) out our feature map and \n",
    "        # putting it through a linear layer we can just use a conv layer\n",
    "        # where the kernal is the same size as the feature map \n",
    "        # (in practice it's the same thing)\n",
    "        self.conv_mu = nn.Conv2d(4 * ch, z, 4, 1)\n",
    "        self.conv_logvar = nn.Conv2d(4 * ch, z, 4, 1)\n",
    "\n",
    "    # This function will sample from our distribution\n",
    "    def sample(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.elu(self.conv_1(x))\n",
    "        \n",
    "        x = self.conv_block1(x)\n",
    "        x = self.conv_block2(x)\n",
    "        x = self.conv_block3(x)\n",
    "\n",
    "        mu = self.conv_mu(x)\n",
    "        logvar = self.conv_logvar(x)\n",
    "        x = self.sample(mu, logvar)\n",
    "        \n",
    "        return x, mu, logvar\n",
    "    \n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, channels, ch = 32, z = 32):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.ConvTranspose2d(z, 4 * ch, 4, 1)\n",
    "        \n",
    "        self.conv_block1 = UpBlock(4 * ch, 2 * ch)\n",
    "        self.conv_block2 = UpBlock(2 * ch, ch)\n",
    "        self.conv_block3 = UpBlock(ch, ch)\n",
    "        self.conv_out = nn.Conv2d(ch, channels, 3, 1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        \n",
    "        x = self.conv_block1(x)\n",
    "        x = self.conv_block2(x)\n",
    "        x = F.elu(self.conv_block3(x))\n",
    "\n",
    "        return torch.tanh(self.conv_out(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, channel_in, ch=16, z=32):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = Encoder(channels=channel_in, ch=ch, z=z)\n",
    "        self.decoder = Decoder(channels=channel_in, ch=ch, z=z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoding, mu, logvar = self.encoder(x)\n",
    "        \n",
    "        # Only sample during training or when we want to generate new images\n",
    "        # just use mu otherwise\n",
    "        if self.training:\n",
    "            x = self.decoder(encoding)\n",
    "        else:\n",
    "            x = self.decoder(mu)\n",
    "            \n",
    "        return x, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a test image\n",
    "dataiter = iter(test_loader)\n",
    "test_images = dataiter._next_data()[0]\n",
    "\n",
    "# View the shape\n",
    "test_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the data!!!\n",
    "plt.figure(figsize = (20,10))\n",
    "out = vutils.make_grid(test_images[0:8], normalize=True)\n",
    "plt.imshow(out.numpy().transpose((1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create our network\n",
    "vae_net = VAE(channel_in=1, z=latent_size).to(device)\n",
    "\n",
    "# Setup optimizer\n",
    "optimizer = optim.Adam(vae_net.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "\n",
    "# Create loss logger\n",
    "loss_log = []\n",
    "train_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Pass through a test image to make sure everything is working\n",
    "recon_data, mu, logvar = vae_net(test_images.to(device))\n",
    "\n",
    "# View the Latent vector shape\n",
    "mu.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbar = trange(0, nepoch, leave=False, desc=\"Epoch\")   \n",
    "vae_net.train()\n",
    "train_loss = 0\n",
    "for epoch in pbar:\n",
    "    pbar.set_postfix_str('Loss: %.4f' % (train_loss/len(train_loader)))\n",
    "    train_loss = 0\n",
    "    for i, data in enumerate(tqdm(train_loader, leave=False, desc=\"Training\")):\n",
    "\n",
    "        image = data[0].to(device)\n",
    "\n",
    "        # Forward pass the image in the data tuple\n",
    "        recon_data, mu, logvar = vae_net(image)\n",
    "        \n",
    "        # Calculate the loss\n",
    "        loss = vae_loss(recon_data, image, mu, logvar)\n",
    "        \n",
    "        # Log the loss\n",
    "        loss_log.append(loss.item())\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Take a training step\n",
    "        vae_net.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.plot(loss_log[1000:])\n",
    "_ = plt.title(\"VAE Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_net.eval()\n",
    "recon_data, mu, logvar = vae_net(test_images.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize = (20,10))\n",
    "out = vutils.make_grid(test_images[0:8], normalize=True)\n",
    "_ = plt.imshow(out.numpy().transpose((1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize = (20,10))\n",
    "out = vutils.make_grid(recon_data.detach().cpu()[0:8], normalize=True)\n",
    "_ = plt.imshow(out.numpy().transpose((1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize = (20,10))\n",
    "out = vutils.make_grid(recon_data.detach().cpu()[0:8], normalize=True)\n",
    "_ = plt.imshow(out.numpy().transpose((1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rand_samp = vae_net.decoder(torch.randn_like(mu))\n",
    "plt.figure(figsize = (20,10))\n",
    "out = vutils.make_grid(rand_samp.detach().cpu()[0:8], normalize=True)\n",
    "_ = plt.imshow(out.numpy().transpose((1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise the class means to 0\n",
    "class_means = torch.zeros(10, latent_size)\n",
    "\n",
    "vae_net.eval()\n",
    "# Loop through all the data in the validation set\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(test_loader, 0):\n",
    "        images, labels = data\n",
    "        recon_data, mu, _ = vae_net(images.to(device))\n",
    "        \n",
    "        # For each batch sum up the latent vectors of the same class\n",
    "        # (Use a matrix of one hot coded vectors to make it easy)\n",
    "        class_matrix = F.one_hot(labels, 10).t().type(torch.FloatTensor)\n",
    "        class_means += torch.matmul(class_matrix, mu.squeeze().detach().cpu())\n",
    "\n",
    "# In the validation set each class has 1000 images so to find the mean vectors we divide by 1000\n",
    "class_means /= 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise the class means to 0\n",
    "class_means = torch.zeros(10, latent_size)\n",
    "\n",
    "vae_net.eval()\n",
    "# Loop through all the data in the validation set\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(test_loader, 0):\n",
    "        images, labels = data\n",
    "        recon_data, mu, _ = vae_net(images.to(device))\n",
    "        \n",
    "        # For each batch sum up the latent vectors of the same class\n",
    "        # (Use a matrix of one hot coded vectors to make it easy)\n",
    "        class_matrix = F.one_hot(labels, 10).t().type(torch.FloatTensor)\n",
    "        class_means += torch.matmul(class_matrix, mu.squeeze().detach().cpu())\n",
    "\n",
    "# In the validation set each class has 1000 images so to find the mean vectors we divide by 1000\n",
    "class_means /= 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick the two classes to move between\n",
    "start_class = 4\n",
    "end_class = 7\n",
    "\n",
    "# Number of interpolation steps\n",
    "num_steps = 100\n",
    "steps = torch.linspace(0,1,num_steps)\n",
    "\n",
    "# Get the vector pointing from one class to the other\n",
    "diff_vector = class_means[end_class] - class_means[start_class]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_steps = class_means[start_class] + (steps.view(num_steps, 1, 1, 1) * diff_vector.view(1, latent_size, 1, 1))\n",
    "recon_steps = vae_net.decoder(latent_steps.to(device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize = (20,10))\n",
    "out = vutils.make_grid(recon_steps.detach().cpu(), normalize=True)\n",
    "plt.imshow(out.numpy().transpose((1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_steps):\n",
    "    plt.imshow(((recon_steps[i, 0] + 1) / 2).detach().cpu())\n",
    "    plt.pause(0.01)\n",
    "    clear_output(True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
