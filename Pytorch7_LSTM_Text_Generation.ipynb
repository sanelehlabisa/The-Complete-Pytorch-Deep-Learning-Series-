{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c131e741",
   "metadata": {},
   "source": [
    "# Text Generation, Many-to-Many with Text.\n",
    "If we treat a sentence as sequence of data-points, like a time series data source, we can perform \"next token\" prediction and create a model that tries to complete the text sequence based on an initial input. This is commonly refered to as \"text generation\".\n",
    "\n",
    "[<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/9/93/LSTM_Cell.svg/2880px-LSTM_Cell.svg.png\">](LSTM)\n",
    "<br>\n",
    "[Corresponding Tutorial Video](https://youtu.be/8K6EL7h9gYI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e84e255",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import io\n",
    "import re\n",
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "from torchtext.datasets import WikiText2, EnWik9, AG_NEWS\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import torchtext.transforms as T\n",
    "from torch.hub import load_state_dict_from_url\n",
    "from torchtext.data.functional import sentencepiece_tokenizer, load_sp_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d70b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "# Step size for parameter updates\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# Number of training epochs\n",
    "nepochs = 20\n",
    "\n",
    "# Number of samples processed together\n",
    "batch_size = 32\n",
    "\n",
    "# Maximum sequence length\n",
    "max_len = 64\n",
    "\n",
    "# Root directory of the dataset\n",
    "data_set_root = \"../../datasets\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6a6001",
   "metadata": {},
   "source": [
    "## Dataset, Tokenizers and Vocab!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7214da7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll be using the AG News Dataset\n",
    "# Which contains a short news article and a single label to classify the \"type\" of article\n",
    "# Note that for torchtext these datasets are NOT Pytorch dataset classes \"AG_NEWS\" is a function that\n",
    "# returns a Pytorch DataPipe!\n",
    "\n",
    "# Pytorch DataPipes vvv\n",
    "# https://pytorch.org/data/main/torchdata.datapipes.iter.html\n",
    "\n",
    "# vvv Good Blog on the difference between DataSet and DataPipe\n",
    "# https://medium.com/deelvin-machine-learning/comparison-of-pytorch-dataset-and-torchdata-datapipes-486e03068c58\n",
    "# Depending on the dataset sometimes the dataset doesn't download and gives an error\n",
    "# and you'll have to download and extract manually \n",
    "# \"The datasets supported by torchtext are datapipes from the torchdata project, which is still in Beta status\"\n",
    "\n",
    "# Un-comment to triger the DataPipe to download the data vvv\n",
    "# dataset_train = AG_NEWS(root=data_set_root, split=\"train\")\n",
    "# data = next(iter(dataset_train))\n",
    "\n",
    "# Side-Note I've noticed that the WikiText dataset is no longer able to be downloaded :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0146ff12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Un-Comment to train sentence-piece model for tokenizer and vocab!\n",
    "\n",
    "# from torchtext.data.functional import generate_sp_model\n",
    "\n",
    "# with open(os.path.join(data_set_root, \"datasets/AG_NEWS/train.csv\")) as f:\n",
    "#     with open(os.path.join(data_set_root, \"datasets/AG_NEWS/data.txt\"), \"w\") as f2:\n",
    "#         for i, line in enumerate(f):\n",
    "#             text_only = \"\".join(line.split(\",\")[1:])\n",
    "#             filtered = re.sub(r'\\\\|\\\\n|;', ' ', text_only.replace('\"', ' ').replace('\\n', ' ')) # remove newline characters\n",
    "#             f2.write(filtered.lower() + \"\\n\")\n",
    "\n",
    "# generate_sp_model(os.path.join(data_set_root, \"datasets/AG_NEWS/data.txt\"), \n",
    "#                   vocab_size=20000, model_prefix='spm_ag_news')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb711b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AGNews dataset class definition\n",
    "class AGNews(Dataset):\n",
    "    def __init__(self, num_datapoints, test_train=\"train\"):\n",
    "        # Read the AG News dataset CSV file based on the test_train parameter (train or test)\n",
    "        self.df = pd.read_csv(os.path.join(data_set_root, \"datasets/AG_NEWS/\" + test_train + \".csv\"),\n",
    "                              names=[\"Class\", \"Title\", \"Content\"])\n",
    "        \n",
    "        # Fill missing values with empty string\n",
    "        self.df.fillna('', inplace=True)\n",
    "        \n",
    "        # Combine Title and Content columns into a single Article column\n",
    "        self.df['Article'] = self.df['Title'] + \" : \" + self.df['Content']\n",
    "        \n",
    "        # Drop Title and Content columns as they are no longer needed\n",
    "        self.df.drop(['Title', 'Content'], axis=1, inplace=True)\n",
    "        \n",
    "        # Replace special characters with whitespace in the Article column\n",
    "        self.df['Article'] = self.df['Article'].str.replace(r'\\\\n|\\\\|\\\\r|\\\\r\\\\n|\\n|\"', ' ', regex=True)\n",
    "\n",
    "    # Method to get a single item from the dataset\n",
    "    def __getitem__(self, index):\n",
    "        # Get the text of the article at the given index, converted to lowercase\n",
    "        text = self.df.loc[index][\"Article\"].lower()\n",
    "\n",
    "        return text\n",
    "\n",
    "    # Method to get the length of the dataset\n",
    "    def __len__(self):\n",
    "        # Return the total number of articles in the dataset\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4417ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create AGNews dataset instances for training and testing\n",
    "dataset_train = AGNews(num_datapoints=data_set_root, test_train=\"train\")\n",
    "dataset_test = AGNews(num_datapoints=data_set_root, test_train=\"test\")\n",
    "\n",
    "# Create data loaders for training and testing datasets\n",
    "# DataLoader for training dataset\n",
    "data_loader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, num_workers=8, drop_last=True)\n",
    "# DataLoader for testing dataset\n",
    "data_loader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=True, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7192529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of using the tokenizer\n",
    "# Load the SentencePiece model\n",
    "sp_model = load_sp_model(\"spm_ag_news.model\")\n",
    "\n",
    "# Create a tokenizer using the loaded model\n",
    "tokenizer = sentencepiece_tokenizer(sp_model)\n",
    "\n",
    "# Iterate over tokens generated by the tokenizer\n",
    "for token in tokenizer([\"i am creating\"]):\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031db2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to yield tokens from a file\n",
    "def yield_tokens(file_path):\n",
    "    # Open the file in UTF-8 encoding\n",
    "    with io.open(file_path, encoding='utf-8') as f:\n",
    "        # Iterate over each line in the file\n",
    "        for line in f:\n",
    "            # Yield the token split by tab character\n",
    "            yield [line.split(\"\\t\")[0]]\n",
    "\n",
    "# Build vocabulary from the iterator of tokens\n",
    "# We will also add \"special\" tokens that we'll use to signal something to our model\n",
    "# <pad> is a padding token that is added to the end of a sentence to ensure \n",
    "# the length of all sequences in a batch is the same\n",
    "# <sos> signals the \"Start-Of-Sentence\" aka the start of the sequence\n",
    "# <eos> signals the \"End-Of-Sentence\" aka the end of the sequence\n",
    "# <unk> \"unknown\" token is used if a token is not contained in the vocab\n",
    "vocab = build_vocab_from_iterator(\n",
    "    yield_tokens(\"spm_ag_news.vocab\"),\n",
    "    # Define special tokens with special_first=True to place them at the beginning of the vocabulary\n",
    "    specials=['<pad>', '<sos>', '<eos>', '<unk>'],\n",
    "    special_first=True\n",
    ")\n",
    "\n",
    "# Set default index for out-of-vocabulary tokens\n",
    "vocab.set_default_index(vocab['<unk>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa2a4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenDrop(nn.Module):\n",
    "    \"\"\"For a batch of tokens indices, randomly replace a non-specical token with <pad>.\n",
    "    \n",
    "    Args:\n",
    "        prob (float): probability of dropping a token\n",
    "        pad_token (int): index for the <pad> token\n",
    "        num_special (int): Number of special tokens, assumed to be at the start of the vocab\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, prob=0.1, pad_token=0, num_special=4):\n",
    "        self.prob = prob\n",
    "        self.num_special = num_special\n",
    "        self.pad_token = pad_token\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        # Randomly sample a bernoulli distribution with p=prob\n",
    "        # to create a mask where 1 means we will replace that token\n",
    "        mask = torch.bernoulli(self.prob * torch.ones_like(sample)).long()\n",
    "        \n",
    "        # only replace if the token is not a special token\n",
    "        can_drop = (sample >= self.num_special).long()\n",
    "        mask = mask * can_drop\n",
    "        \n",
    "        replace_with = (self.pad_token * torch.ones_like(sample)).long()\n",
    "        \n",
    "        sample_out = (1 - mask) * sample + mask * replace_with\n",
    "        \n",
    "        return sample_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72346ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a transformation pipeline for training data\n",
    "train_transform = T.Sequential(\n",
    "    # Tokenize sentences using pre-existing SentencePiece tokenizer model\n",
    "    T.SentencePieceTokenizer(\"spm_ag_news.model\"),\n",
    "    # Convert tokens to indices based on given vocabulary\n",
    "    T.VocabTransform(vocab=vocab),\n",
    "    # Add <sos> token at the beginning of each sentence (index 1 in vocabulary)\n",
    "    T.AddToken(1, begin=True),\n",
    "    # Crop the sentence if it is longer than the max length\n",
    "    T.Truncate(max_seq_len=max_len),\n",
    "    # Add <eos> token at the end of each sentence (index 2 in vocabulary)\n",
    "    T.AddToken(2, begin=False),\n",
    "    # Convert the list of lists to a tensor and pad sentences with the <pad> token if shorter than max length\n",
    "    T.ToTensor(padding_value=0)\n",
    ")\n",
    "\n",
    "# Define a transformation pipeline for generation (without truncation)\n",
    "gen_transform = T.Sequential(\n",
    "    # Tokenize sentences using pre-existing SentencePiece tokenizer model\n",
    "    T.SentencePieceTokenizer(\"spm_ag_news.model\"),\n",
    "    # Convert tokens to indices based on given vocabulary\n",
    "    T.VocabTransform(vocab=vocab),\n",
    "    # Add <sos> token at the beginning of each sentence (index 1 in vocabulary)\n",
    "    T.AddToken(1, begin=True),\n",
    "    # Convert the list of lists to a tensor and pad sentences with the <pad> token if shorter than max length\n",
    "    T.ToTensor(padding_value=0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a4b1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = next(iter(data_loader_train))\n",
    "index = 0\n",
    "input_tokens = train_tranform(text)\n",
    "print(\"SENTENCE\")\n",
    "print(text[index])\n",
    "print()\n",
    "print(\"TOKENS\")\n",
    "print(vocab.lookup_tokens(input_tokens[index].numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460c6c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TOKENS BACK TO SENTENCE\")\n",
    "\n",
    "pred_text = \"\".join(vocab.lookup_tokens(input_tokens[index].numpy()))\n",
    "pred_text.replace(\"▁\", \" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85358fa0",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350ac2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, num_emb, num_layers=1, emb_size=128, hidden_size=128):\n",
    "        super(LSTM, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(num_emb, emb_size)\n",
    "\n",
    "        self.mlp_emb = nn.Sequential(nn.Linear(emb_size, emb_size),\n",
    "                                     nn.LayerNorm(emb_size),\n",
    "                                     nn.ELU(),\n",
    "                                     nn.Linear(emb_size, emb_size))\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=emb_size, hidden_size=hidden_size, \n",
    "                            num_layers=num_layers, batch_first=True, dropout=0.25)\n",
    "\n",
    "        self.mlp_out = nn.Sequential(nn.Linear(hidden_size, hidden_size//2),\n",
    "                                     nn.LayerNorm(hidden_size//2),\n",
    "                                     nn.ELU(),\n",
    "                                     nn.Dropout(0.5),\n",
    "                                     nn.Linear(hidden_size//2, num_emb))\n",
    "        \n",
    "    def forward(self, input_seq, hidden_in, mem_in):\n",
    "        input_embs = self.embedding(input_seq)\n",
    "        input_embs = self.mlp_emb(input_embs)\n",
    "                \n",
    "        output, (hidden_out, mem_out) = self.lstm(input_embs, (hidden_in, mem_in))\n",
    "                \n",
    "        return self.mlp_out(output), hidden_out, mem_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc60be97",
   "metadata": {},
   "source": [
    "## Initialise Model and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6341169c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available, set device accordingly\n",
    "device = torch.device(1 if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Define embedding size and hidden size for the LSTM model\n",
    "emb_size = 256\n",
    "hidden_size = 1024\n",
    "\n",
    "# Define number of layers for the LSTM model\n",
    "num_layers = 2\n",
    "\n",
    "# Create LSTM model instance\n",
    "lstm_generator = LSTM(num_emb=len(vocab), num_layers=num_layers, \n",
    "                      emb_size=emb_size, hidden_size=hidden_size).to(device)\n",
    "\n",
    "# Initialize optimizer with Adam optimizer\n",
    "optimizer = optim.Adam(lstm_generator.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "\n",
    "# Define the loss function (Cross Entropy Loss)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Custom transform that will randomly replace a token with <pad>\n",
    "td = TokenDrop(prob=0.1)\n",
    "\n",
    "# List to store training loss during each epoch\n",
    "training_loss_logger = []\n",
    "\n",
    "# List to store entropy during training (for monitoring)\n",
    "entropy_logger = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bdb0ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how many Parameters our Model has!\n",
    "num_model_params = 0\n",
    "for param in lstm_generator.parameters():\n",
    "    num_model_params += param.flatten().shape[0]\n",
    "\n",
    "print(\"-This Model Has %d (Approximately %d Million) Parameters!\" % (num_model_params, num_model_params//1e6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2bc952",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd56a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "for epoch in trange(0, nepochs, leave=False, desc=\"Epoch\"):\n",
    "    # Set LSTM generator model to training mode\n",
    "    lstm_generator.train()\n",
    "    steps = 0\n",
    "    # Iterate over batches in training data loader\n",
    "    for text in tqdm(data_loader_train, desc=\"Training\", leave=False):\n",
    "        # Transform text tokens using training transform and move to device\n",
    "        text_tokens = train_transform(list(text)).to(device)\n",
    "        bs = text_tokens.shape[0]\n",
    "        \n",
    "        # Randomly drop input tokens\n",
    "        input_text = td(text_tokens[:, 0:-1])\n",
    "        output_text = text_tokens[:, 1:]\n",
    "        \n",
    "        # Initialize the memory buffers\n",
    "        hidden = torch.zeros(num_layers, bs, hidden_size, device=device)\n",
    "        memory = torch.zeros(num_layers, bs, hidden_size, device=device)\n",
    "        \n",
    "        # Forward pass through the LSTM generator\n",
    "        pred, hidden, memory = lstm_generator(input_text, hidden, memory)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = loss_fn(pred.transpose(1, 2), output_text)\n",
    "        \n",
    "        # Zero gradients, perform backward pass, and update weights\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Log training loss\n",
    "        training_loss_logger.append(loss.item())\n",
    "        \n",
    "        # Log entropy during training (for monitoring)\n",
    "        with torch.no_grad():\n",
    "            dist = Categorical(logits=pred)\n",
    "            entropy_logger.append(dist.entropy().mean().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eea879c",
   "metadata": {},
   "source": [
    "## Plot Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5dde70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.figure(figsize=(10, 5))\n",
    "_ = plt.plot(training_loss_logger[100:])\n",
    "_ = plt.title(\"Training Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3bf293",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.figure(figsize=(10, 5))\n",
    "_ = plt.plot(entropy_logger[1000:])\n",
    "_ = plt.title(\"Distribution Entropy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e4b610",
   "metadata": {},
   "source": [
    "## Generate some text!\n",
    "Lets use the fact that all of the articles have the title and content seperated by a : to get our model to generate some content based on a title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecf35c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get some test examples\n",
    "text = next(iter(data_loader_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecfeb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose an index from the test data loader\n",
    "index = 0\n",
    "\n",
    "# Temperature parameter for sampling\n",
    "temp = 0.9\n",
    "\n",
    "# We can either use an example from the test set or create our own article title!\n",
    "# init_prompt = [\"the next big thing from google :\"]\n",
    "init_prompt = [text[index].split(\":\")[0] + \":\"]\n",
    "\n",
    "# Transform the initial prompt into tokens and move to device\n",
    "input_tokens = gen_transform(init_prompt).to(device)\n",
    "\n",
    "print(\"INITIAL PROMPT:\")\n",
    "print(init_prompt[0])\n",
    "\n",
    "print(\"\\nPROMPT TOKENS:\")\n",
    "print(input_tokens)\n",
    "print(vocab.lookup_tokens(input_tokens[0].cpu().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6148b641",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_tokens = []\n",
    "\n",
    "# Set LSTM generator model to evaluation mode\n",
    "lstm_generator.eval()\n",
    "\n",
    "# Disable gradient calculation\n",
    "with torch.no_grad():    \n",
    "    # Initialize hidden and memory states\n",
    "    hidden = torch.zeros(num_layers, 1, hidden_size, device=device)\n",
    "    memory = torch.zeros(num_layers, 1, hidden_size, device=device)\n",
    "    \n",
    "    # Generate text\n",
    "    for i in range(100):\n",
    "        # Forward pass through LSTM generator\n",
    "        data_pred, hidden, memory = lstm_generator(input_tokens, hidden, memory)\n",
    "        \n",
    "        # Sample from the distribution of probabilities (with temperature)\n",
    "        dist = Categorical(logits=data_pred[:, -1] / temp)\n",
    "        input_tokens = dist.sample().reshape(1, 1)\n",
    "        \n",
    "        # Append generated token to log_tokens\n",
    "        log_tokens.append(input_tokens.cpu())\n",
    "        \n",
    "        # Check for end-of-sentence token\n",
    "        if input_tokens.item() == 2:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024ddad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets look at the raw tokens\n",
    "pred_text = \"\".join(vocab.lookup_tokens(torch.cat(log_tokens, 1)[0].numpy()))\n",
    "print(pred_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286c2f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the model's output with the initial title to get our article!\n",
    "init_prompt[0] + pred_text.replace(\"▁\", \" \").replace(\"<unk>\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b626aacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets have a look at the probabilities\n",
    "# _ = plt.plot(F.softmax(data_pred/temp, -1).cpu().numpy().flatten())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
