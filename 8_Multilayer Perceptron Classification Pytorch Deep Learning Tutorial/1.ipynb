{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-Layer Perception Classification of MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data.dataloader as dataloader\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "import torchvision\n",
    "\n",
    "from tqdm.notebook import trange, tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ../../datasets\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9.91M/9.91M [00:06<00:00, 1.61MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../datasets\\MNIST\\raw\\train-images-idx3-ubyte.gz to ../../datasets\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ../../datasets\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28.9k/28.9k [00:00<00:00, 62.4kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../datasets\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ../../datasets\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ../../datasets\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1.65M/1.65M [00:07<00:00, 230kB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../datasets\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ../../datasets\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ../../datasets\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4.54k/4.54k [00:00<00:00, 4.49MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../datasets\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ../../datasets\\MNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "data_set_root = \"../datasets\"\n",
    "\n",
    "# Create a train and test dataset using the Pytorch MNIST dataloader class\n",
    "train = MNIST(data_set_root, train=True,  download=True, transform=transforms.ToTensor())\n",
    "test  = MNIST(data_set_root, train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "# Using the Pytorch dataloader class and the Pytorch datasets we with create itterable dataloader objects\n",
    "train_loader = dataloader.DataLoader(train, shuffle=True, batch_size=batch_size, num_workers=0, pin_memory=False) \n",
    "test_loader = dataloader.DataLoader(test, shuffle=False, batch_size=batch_size, num_workers=0, pin_memory=False)\n",
    "\n",
    "# NOTE:num_workers is the number of extra threads the dataloader will spawn to load the data from file, \n",
    "# you will rarely need more than 4 \n",
    "# NOTE!!! ON WINDOWS THERE CAN BE ISSUES WITH HAVING MORE THAN 0 WORKERS!! IF YOUR TRAINING LOOP STALLS AND DOES\n",
    "# NOTHING SET num_workers TO 0!\n",
    "\n",
    "# NOTE:pin_memory is only useful if you are training with a GPU!!!! If it is True then the GPU will pre-allocate\n",
    "# memory for the NEXT batch so the CPU-GPU transfer can be handled by the DMA controller freeing up the CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device to GPU_indx if GPU is avaliable\n",
    "# GPU index is set by the NVIDIA Drivers if you have only one GPU then it should always be 0\n",
    "GPU_indx = 0\n",
    "device = torch.device(GPU_indx if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The input data shape is :\n",
      " torch.Size([256, 1, 28, 28])\n",
      "The target output data shape is :\n",
      " torch.Size([256])\n"
     ]
    }
   ],
   "source": [
    "images, labels = next(iter(train_loader))\n",
    "print(\"The input data shape is :\\n\", images.shape)\n",
    "print(\"The target output data shape is :\\n\", labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The input data shape is :\n",
      " torch.Size([256, 1, 28, 28])\n",
      "The target output data shape is :\n",
      " torch.Size([256])\n"
     ]
    }
   ],
   "source": [
    "images, labels = next(iter(train_loader))\n",
    "print(\"The input data shape is :\\n\", images.shape)\n",
    "print(\"The target output data shape is :\\n\", labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets create a simple MLP network similar to the sine wave approximator\n",
    "class Simple_MLP(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(Simple_MLP, self).__init__()\n",
    "        # We will use 4 linear layers\n",
    "        # The input to the model is 784 (28x28 - the image size)\n",
    "        # and the should be num_classes outputs\n",
    "        self.fc1 = nn.Linear(784, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        self.fc4 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # The data we pass the model is a batch of single channel images\n",
    "        # with shape BSx1x28x28 we need to flatten it to BSx784\n",
    "        # To use it in a linear layer\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        \n",
    "        # We will use a relu activation function for this network! (F.relu)\n",
    "        # NOTE F.relu is the \"functional\" version of the activation function!\n",
    "        # nn.ReLU is a class constructor of a \"ReLU\" object\n",
    "        # These two things are the same for MOST purposes!\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple_MLP(\n",
      "  (fc1): Linear(in_features=784, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (fc3): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (fc4): Linear(in_features=128, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Create our model\n",
    "model = Simple_MLP(10).to(device)\n",
    "# Create our loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Define our loss funcition and optimizer\n",
    "lr = 1e-3\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Number of Epochs\n",
    "n_epochs = 20\n",
    "\n",
    "# We can print out our model structure\n",
    "print(model)\n",
    "# Note: this is only the order in which the layers were defined NOT the path of the forward pass!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, criterion, optimizer, loss_logger):\n",
    "    for batch_idx, (data, target) in enumerate(tqdm(train_loader, desc=\"Training\", leave=False)):   \n",
    "        outputs = model(data.to(device))\n",
    "        loss = criterion(outputs, target.to(device))\n",
    "        \n",
    "        optimizer.zero_grad()   \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_logger.append(loss.item())\n",
    "\n",
    "    return model, optimizer, loss_logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, criterion, optimizer, loss_logger):\n",
    "    for batch_idx, (data, target) in enumerate(tqdm(train_loader, desc=\"Training\", leave=False)):   \n",
    "        outputs = model(data.to(device))\n",
    "        loss = criterion(outputs, target.to(device))\n",
    "        \n",
    "        optimizer.zero_grad()   \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_logger.append(loss.item())\n",
    "\n",
    "    return model, optimizer, loss_logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_loss = []\n",
    "test_loss  = []\n",
    "test_acc   = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e122be9a9bc545be8cc50132a538bb5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eccded63c24d4455b9b649fa420598b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'test_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m trange(n_epochs, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch\u001b[39m\u001b[38;5;124m\"\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m      2\u001b[0m     model, optimizer, train_loss \u001b[38;5;241m=\u001b[39m train_epoch(model, train_loader, criterion, optimizer, train_loss)\n\u001b[1;32m----> 3\u001b[0m     test_loss, acc \u001b[38;5;241m=\u001b[39m \u001b[43mtest_model\u001b[49m(model, test_loader, criterion, test_loss)\n\u001b[0;32m      4\u001b[0m     test_acc\u001b[38;5;241m.\u001b[39mappend(acc)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinal Accuracy: \u001b[39m\u001b[38;5;132;01m%.2f\u001b[39;00m\u001b[38;5;132;01m%%\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m acc)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_model' is not defined"
     ]
    }
   ],
   "source": [
    "for i in trange(n_epochs, desc=\"Epoch\", leave=False):\n",
    "    model, optimizer, train_loss = train_epoch(model, train_loader, criterion, optimizer, train_loss)\n",
    "    test_loss, acc = test_model(model, test_loader, criterion, test_loss)\n",
    "    test_acc.append(acc)\n",
    "    \n",
    "print(\"Final Accuracy: %.2f%%\" % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = model(images.to(device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(np.arange(10), F.softmax(x, 1)[0].detach().cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epoch Number')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Test Loss')\n",
    "plt.xlabel('Epoch Number')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(test_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.title('Test Accuracy')\n",
    "plt.xlabel('Epoch Number')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.plot(test_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
