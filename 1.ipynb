{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c6e233c",
   "metadata": {},
   "source": [
    "# Predicting Sequential Data With an LSTM\n",
    "In this notebook we introduce the [Long-Short-Term-Memory](https://youtu.be/YCzL96nL7j0?si=Pt5Es_-LVdtEFsDB) block, a RNN that is designed to intelligently pass information to itself, allowing for the processing longer sequences.\n",
    "\n",
    "[<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/9/93/LSTM_Cell.svg/2880px-LSTM_Cell.svg.png\">](LSTM)\n",
    "<br>\n",
    "[Corresponding Tutorial Video](https://youtu.be/lyUT6dOARGs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271571fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm.notebook import trange, tqdm\n",
    "from Dataset import WeatherDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9d44eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the CSV file containing the weather dataset\n",
    "dataset_file = \"../data/weather.csv\"\n",
    "\n",
    "# Define the date to split the dataset into training and testing sets\n",
    "split_date = pd.to_datetime('2023-01-01')\n",
    "\n",
    "# Number of days in the input sequence\n",
    "day_range = 30\n",
    "\n",
    "# Number of days the MLP will take as input\n",
    "days_in = 14\n",
    "\n",
    "# Ensure that the total number of days in the input sequence is larger than the MLP input size\n",
    "assert day_range > days_in, \"The total day range must be larger than the input days for the MLP\"\n",
    "\n",
    "# Define the hyperparameters for training the model\n",
    "learning_rate = 1e-4  # Learning rate for the optimizer\n",
    "nepochs = 500  # Number of training epochs\n",
    "batch_size = 32  # Batch size for training\n",
    "\n",
    "# Create training dataset\n",
    "# This will load the weather data, consider sequences of length day_range,\n",
    "# and split the data such that data before split_date is used for training\n",
    "dataset_train = WeatherDataset(dataset_file, day_range=day_range, split_date=split_date, train_test=\"train\")\n",
    "\n",
    "# Create testing dataset\n",
    "# This will load the weather data, consider sequences of length day_range,\n",
    "# and split the data such that data after split_date is used for testing\n",
    "dataset_test = WeatherDataset(dataset_file, day_range=day_range, split_date=split_date, train_test=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274a28e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of training examples: {len(dataset_train)}')\n",
    "print(f'Number of testing examples: {len(dataset_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfcf18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader_train = DataLoader(dataset=dataset_train, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "data_loader_test = DataLoader(dataset=dataset_test, batch_size=batch_size, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de64576",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 5))\n",
    "_ = plt.title(\"Melbourne Max Daily Temperature (C)\")\n",
    "\n",
    "_ = plt.plot(dataset_train.dataset.index, dataset_train.dataset.values[:, 1])\n",
    "_ = plt.plot(dataset_test.dataset.index, dataset_test.dataset.values[:, 1])\n",
    "\n",
    "_ = plt.legend([\"Train\", \"Test\"])\n",
    "# Note:see here how we can just directly access the data from the dataset class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbedceba",
   "metadata": {},
   "source": [
    "## Create LSTM Model\n",
    "This is a network class definition that utilizes an LSTM (Long Short-Term Memory) architecture for sequential data prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74309b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlockMLP(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(ResBlockMLP, self).__init__()\n",
    "        # Define the layers for the residual block\n",
    "        self.norm1 = nn.LayerNorm(input_size)\n",
    "        self.fc1 = nn.Linear(input_size, input_size // 2)\n",
    "        \n",
    "        self.norm2 = nn.LayerNorm(input_size // 2)\n",
    "        self.fc2 = nn.Linear(input_size // 2, output_size)\n",
    "        \n",
    "        self.fc3 = nn.Linear(input_size, output_size)\n",
    "\n",
    "        self.act = nn.ELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass of the residual block\n",
    "        x = self.act(self.norm1(x))\n",
    "        skip = self.fc3(x)\n",
    "        \n",
    "        x = self.act(self.norm2(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x + skip\n",
    "\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, seq_len, output_size, num_blocks=1):\n",
    "        super(LSTM, self).__init__()\n",
    "        # Calculate the length of the sequence data\n",
    "        seq_data_len = seq_len * 2\n",
    "        # Define the layers for the input MLP\n",
    "        self.input_mlp = nn.Sequential(nn.Linear(seq_data_len, 4 * seq_data_len),\n",
    "                                       nn.ELU(),\n",
    "                                       nn.Linear(4 * seq_data_len, 128))\n",
    "        # Create an LSTM block\n",
    "        self.lstm = nn.LSTM(input_size=128, hidden_size=128, num_layers=1)\n",
    "        \n",
    "        # Define residual blocks\n",
    "        blocks = [ResBlockMLP(128, 128) for _ in range(num_blocks)]\n",
    "        self.res_blocks = nn.Sequential(*blocks)\n",
    "        \n",
    "        # Output fully connected layer\n",
    "        self.fc_out = nn.Linear(128, output_size)\n",
    "        self.act = nn.ELU()\n",
    "\n",
    "    def forward(self, input_seq, hidden_in, mem_in):\n",
    "        # Reshape input sequence\n",
    "        input_seq = input_seq.reshape(input_seq.shape[0], -1)\n",
    "        \n",
    "        # Pass input sequence through the input MLP\n",
    "        input_vec = self.input_mlp(input_seq).unsqueeze(0)\n",
    "        \n",
    "        # Pass input through LSTM block\n",
    "        output, (hidden_out, mem_out) = self.lstm(input_vec, (hidden_in, mem_in))\n",
    "        \n",
    "        # Pass LSTM output through residual blocks\n",
    "        x = self.act(self.res_blocks(output)).squeeze(0)\n",
    "        \n",
    "        # Pass through output fully connected layer\n",
    "        return self.fc_out(x), hidden_out, mem_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f29d0a",
   "metadata": {},
   "source": [
    "##  Initialize LSTM and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ccc1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the device to GPU if available, otherwise fallback to CPU\n",
    "device = torch.device(0 if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Create the LSTM model\n",
    "weather_lstm = LSTM(seq_len=days_in, output_size=2).to(device)\n",
    "\n",
    "# Initialize the optimizer with the specified learning rate and model parameters\n",
    "optimizer = optim.Adam(weather_lstm.parameters(), lr=learning_rate)\n",
    "\n",
    "# Define the loss function (mean squared error)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Initialize an empty list to log training loss\n",
    "training_loss_logger = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c4cac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how many Parameters our Model has!\n",
    "num_model_params = 0\n",
    "for param in weather_rnn.parameters():\n",
    "    num_model_params += param.flatten().shape[0]\n",
    "\n",
    "print(\"-This Model Has %d (Approximately %d Million) Parameters!\" % (num_model_params, num_model_params//1e6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bc1d9c",
   "metadata": {},
   "source": [
    "## Training\n",
    "Lets train our LSTM model (weather_lstm) using the data loader (data_loader_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86ef651",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in trange(nepochs, desc=\"Epochs\", leave=False):\n",
    "    # Set the model to training mode\n",
    "    weather_lstm.train()\n",
    "    \n",
    "    # Iterate over batches in the training data loader\n",
    "    for day, month, data_seq in tqdm(data_loader_train, desc=\"Training\", leave=False):\n",
    "        # Extract input sequence block\n",
    "        seq_block = data_seq[:, :days_in].to(device)\n",
    "        \n",
    "        # Initialize hidden state and memory tensors with zeros\n",
    "        hidden = torch.zeros(1, data_seq.shape[0], 128, device=device)\n",
    "        memory = torch.zeros(1, data_seq.shape[0], 128, device=device)\n",
    "\n",
    "        # Initialize loss value\n",
    "        loss = 0\n",
    "        \n",
    "        # Iterate over the sequence steps\n",
    "        for i in range(day_range - days_in):\n",
    "            # Extract target sequence block\n",
    "            target_seq_block = data_seq[:, i + days_in].to(device)\n",
    "            \n",
    "            # Forward pass: predict using the input sequence block and update hidden states\n",
    "            data_pred, hidden, memory = weather_lstm(seq_block, hidden, memory)\n",
    "            \n",
    "            # Accumulate the loss\n",
    "            loss += loss_fn(data_pred, target_seq_block)\n",
    "\n",
    "            # Update input sequence block by removing the oldest prediction and adding the new prediction\n",
    "            seq_block = torch.cat((seq_block[:, 1:, :], data_pred.unsqueeze(1).detach()), 1)\n",
    "        \n",
    "        # Calculate the average loss\n",
    "        loss /= i + 1\n",
    "        \n",
    "        # Backpropagation: compute gradients and update model parameters\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Log the training loss\n",
    "        training_loss_logger.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b5978e",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.figure(figsize=(10, 5))\n",
    "_ = plt.plot(training_loss_logger)\n",
    "_ = plt.title(\"Training Loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd974dc",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8697a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tensor = torch.FloatTensor(dataset_test.dataset.values)\n",
    "\n",
    "log_predictions = []  # List to store predicted weather values\n",
    "weather_lstm.eval()  # Set the model to evaluation mode\n",
    "\n",
    "with torch.no_grad():\n",
    "    seq_block = data_tensor[:days_in, :].unsqueeze(0).to(device)  # Extract initial sequence block\n",
    "    \n",
    "    # Initialize hidden state and memory tensors with zeros\n",
    "    hidden = torch.zeros(1, seq_block.shape[0], 128, device=device)\n",
    "    memory = torch.zeros(1, seq_block.shape[0], 128, device=device)\n",
    "    \n",
    "    # Iterate over the remaining days in the test dataset\n",
    "    for i in range(data_tensor.shape[0] - days_in):\n",
    "        # Predict the next day's weather using the LSTM model\n",
    "        data_pred, hidden, memory = weather_lstm(seq_block, hidden, memory)\n",
    "        \n",
    "        # Update the input sequence block by removing the oldest prediction and adding the new prediction\n",
    "        seq_block = torch.cat((seq_block[:, 1:, :], data_pred.unsqueeze(1)), 1)\n",
    "        \n",
    "        # Append the predicted weather values to the list\n",
    "        log_predictions.append(data_pred.cpu())\n",
    "\n",
    "# Concatenate all predicted weather values\n",
    "predictions_cat = torch.cat(log_predictions)\n",
    "\n",
    "# Convert the predictions back to the original scale (undo normalization)\n",
    "un_norm_predictions = (predictions_cat * dataset_test.std) + dataset_test.mean\n",
    "\n",
    "# Convert the original test data back to the original scale\n",
    "un_norm_data = (data_tensor * dataset_test.std) + dataset_test.mean\n",
    "\n",
    "# Skip the initial days (as they were used for initialization) and keep the ground truth data\n",
    "un_norm_data = un_norm_data[days_in:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965b8e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mse = (un_norm_data - un_norm_predictions).pow(2).mean().item()\n",
    "print(\"Test MSE value %.2f\" % test_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89e856f",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.figure(figsize=(10, 5))\n",
    "_ = plt.plot(un_norm_data[:, 0])\n",
    "_ = plt.plot(un_norm_predictions[:, 0])\n",
    "_ = plt.title(\"Rainfall (mm)\")\n",
    "\n",
    "_ = plt.legend([\"Ground Truth\", \"Prediction\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c8d7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.figure(figsize=(10, 5))\n",
    "_ = plt.plot(un_norm_data[:, 1])\n",
    "_ = plt.plot(un_norm_predictions[:, 1])\n",
    "_ = plt.title(\"Max Daily Temperature (C)\")\n",
    "\n",
    "_ = plt.legend([\"Ground Truth\", \"Prediction\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
